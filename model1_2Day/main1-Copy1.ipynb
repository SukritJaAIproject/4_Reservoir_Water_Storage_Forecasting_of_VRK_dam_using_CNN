{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# from func1 import *\n",
    "# from query2 import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data_Historian01.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>MW</th>\n",
       "      <th>80GAW</th>\n",
       "      <th>26WIS</th>\n",
       "      <th>26WOS</th>\n",
       "      <th>dTemp</th>\n",
       "      <th>k</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-13 15:00:00</td>\n",
       "      <td>70.10</td>\n",
       "      <td>4854.22</td>\n",
       "      <td>29.05</td>\n",
       "      <td>26.30</td>\n",
       "      <td>2.75</td>\n",
       "      <td>249.86</td>\n",
       "      <td>926.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-13 16:00:00</td>\n",
       "      <td>70.04</td>\n",
       "      <td>4868.03</td>\n",
       "      <td>29.24</td>\n",
       "      <td>26.36</td>\n",
       "      <td>2.88</td>\n",
       "      <td>249.86</td>\n",
       "      <td>974.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-13 17:00:00</td>\n",
       "      <td>41.11</td>\n",
       "      <td>3103.07</td>\n",
       "      <td>26.77</td>\n",
       "      <td>29.08</td>\n",
       "      <td>2.32</td>\n",
       "      <td>249.86</td>\n",
       "      <td>498.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-13 18:00:00</td>\n",
       "      <td>48.83</td>\n",
       "      <td>2978.46</td>\n",
       "      <td>26.83</td>\n",
       "      <td>29.31</td>\n",
       "      <td>2.48</td>\n",
       "      <td>249.86</td>\n",
       "      <td>512.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-13 19:00:00</td>\n",
       "      <td>93.21</td>\n",
       "      <td>4890.02</td>\n",
       "      <td>29.93</td>\n",
       "      <td>27.21</td>\n",
       "      <td>2.72</td>\n",
       "      <td>249.86</td>\n",
       "      <td>923.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>2020-06-29 22:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>2020-06-29 23:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>2020-06-30 00:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>2020-06-30 01:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3299</th>\n",
       "      <td>2020-06-30 02:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3300 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime     MW    80GAW  26WIS  26WOS  dTemp       k       Q\n",
       "0     2020-02-13 15:00:00  70.10  4854.22  29.05  26.30   2.75  249.86  926.20\n",
       "1     2020-02-13 16:00:00  70.04  4868.03  29.24  26.36   2.88  249.86  974.52\n",
       "2     2020-02-13 17:00:00  41.11  3103.07  26.77  29.08   2.32  249.86  498.87\n",
       "3     2020-02-13 18:00:00  48.83  2978.46  26.83  29.31   2.48  249.86  512.39\n",
       "4     2020-02-13 19:00:00  93.21  4890.02  29.93  27.21   2.72  249.86  923.95\n",
       "...                   ...    ...      ...    ...    ...    ...     ...     ...\n",
       "3295  2020-06-29 22:00:00   0.00     0.00   0.00   0.00   0.00  249.86    0.00\n",
       "3296  2020-06-29 23:00:00   0.00     0.00   0.00   0.00   0.00  249.86    0.00\n",
       "3297  2020-06-30 00:00:00   0.00     0.00   0.00   0.00   0.00  249.86    0.00\n",
       "3298  2020-06-30 01:00:00   0.00     0.00   0.00   0.00   0.00  249.86    0.00\n",
       "3299  2020-06-30 02:00:00   0.00     0.00   0.00   0.00   0.00  249.86    0.00\n",
       "\n",
       "[3300 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3300, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.index.astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3300, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>MW</th>\n",
       "      <th>80GAW</th>\n",
       "      <th>26WIS</th>\n",
       "      <th>26WOS</th>\n",
       "      <th>dTemp</th>\n",
       "      <th>k</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-13 15:00:00</th>\n",
       "      <td>2020-02-13 15:00:00</td>\n",
       "      <td>70.10</td>\n",
       "      <td>4854.22</td>\n",
       "      <td>29.05</td>\n",
       "      <td>26.30</td>\n",
       "      <td>2.75</td>\n",
       "      <td>249.86</td>\n",
       "      <td>926.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13 16:00:00</th>\n",
       "      <td>2020-02-13 16:00:00</td>\n",
       "      <td>70.04</td>\n",
       "      <td>4868.03</td>\n",
       "      <td>29.24</td>\n",
       "      <td>26.36</td>\n",
       "      <td>2.88</td>\n",
       "      <td>249.86</td>\n",
       "      <td>974.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13 17:00:00</th>\n",
       "      <td>2020-02-13 17:00:00</td>\n",
       "      <td>41.11</td>\n",
       "      <td>3103.07</td>\n",
       "      <td>26.77</td>\n",
       "      <td>29.08</td>\n",
       "      <td>2.32</td>\n",
       "      <td>249.86</td>\n",
       "      <td>498.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13 18:00:00</th>\n",
       "      <td>2020-02-13 18:00:00</td>\n",
       "      <td>48.83</td>\n",
       "      <td>2978.46</td>\n",
       "      <td>26.83</td>\n",
       "      <td>29.31</td>\n",
       "      <td>2.48</td>\n",
       "      <td>249.86</td>\n",
       "      <td>512.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13 19:00:00</th>\n",
       "      <td>2020-02-13 19:00:00</td>\n",
       "      <td>93.21</td>\n",
       "      <td>4890.02</td>\n",
       "      <td>29.93</td>\n",
       "      <td>27.21</td>\n",
       "      <td>2.72</td>\n",
       "      <td>249.86</td>\n",
       "      <td>923.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29 22:00:00</th>\n",
       "      <td>2020-06-29 22:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29 23:00:00</th>\n",
       "      <td>2020-06-29 23:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30 00:00:00</th>\n",
       "      <td>2020-06-30 00:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30 01:00:00</th>\n",
       "      <td>2020-06-30 01:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30 02:00:00</th>\n",
       "      <td>2020-06-30 02:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>249.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3300 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                datetime     MW    80GAW  26WIS  26WOS  dTemp  \\\n",
       "datetime                                                                        \n",
       "2020-02-13 15:00:00  2020-02-13 15:00:00  70.10  4854.22  29.05  26.30   2.75   \n",
       "2020-02-13 16:00:00  2020-02-13 16:00:00  70.04  4868.03  29.24  26.36   2.88   \n",
       "2020-02-13 17:00:00  2020-02-13 17:00:00  41.11  3103.07  26.77  29.08   2.32   \n",
       "2020-02-13 18:00:00  2020-02-13 18:00:00  48.83  2978.46  26.83  29.31   2.48   \n",
       "2020-02-13 19:00:00  2020-02-13 19:00:00  93.21  4890.02  29.93  27.21   2.72   \n",
       "...                                  ...    ...      ...    ...    ...    ...   \n",
       "2020-06-29 22:00:00  2020-06-29 22:00:00   0.00     0.00   0.00   0.00   0.00   \n",
       "2020-06-29 23:00:00  2020-06-29 23:00:00   0.00     0.00   0.00   0.00   0.00   \n",
       "2020-06-30 00:00:00  2020-06-30 00:00:00   0.00     0.00   0.00   0.00   0.00   \n",
       "2020-06-30 01:00:00  2020-06-30 01:00:00   0.00     0.00   0.00   0.00   0.00   \n",
       "2020-06-30 02:00:00  2020-06-30 02:00:00   0.00     0.00   0.00   0.00   0.00   \n",
       "\n",
       "                          k       Q  \n",
       "datetime                             \n",
       "2020-02-13 15:00:00  249.86  926.20  \n",
       "2020-02-13 16:00:00  249.86  974.52  \n",
       "2020-02-13 17:00:00  249.86  498.87  \n",
       "2020-02-13 18:00:00  249.86  512.39  \n",
       "2020-02-13 19:00:00  249.86  923.95  \n",
       "...                     ...     ...  \n",
       "2020-06-29 22:00:00  249.86    0.00  \n",
       "2020-06-29 23:00:00  249.86    0.00  \n",
       "2020-06-30 00:00:00  249.86    0.00  \n",
       "2020-06-30 01:00:00  249.86    0.00  \n",
       "2020-06-30 02:00:00  249.86    0.00  \n",
       "\n",
       "[3300 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(('datetime'), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scaler = MinMaxScaler()\n",
    "y_scaler.fit(df[['Q']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['MW','80GAW','26WIS','26WOS','dTemp','k','Q']] = X_scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>80GAW</th>\n",
       "      <th>26WIS</th>\n",
       "      <th>26WOS</th>\n",
       "      <th>dTemp</th>\n",
       "      <th>k</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-13 15:00:00</th>\n",
       "      <td>0.758238</td>\n",
       "      <td>0.929030</td>\n",
       "      <td>0.896882</td>\n",
       "      <td>0.806007</td>\n",
       "      <td>0.092499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13 16:00:00</th>\n",
       "      <td>0.757611</td>\n",
       "      <td>0.931673</td>\n",
       "      <td>0.902748</td>\n",
       "      <td>0.807846</td>\n",
       "      <td>0.096872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.639797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13 17:00:00</th>\n",
       "      <td>0.454964</td>\n",
       "      <td>0.593884</td>\n",
       "      <td>0.826490</td>\n",
       "      <td>0.891204</td>\n",
       "      <td>0.078036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.327521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13 18:00:00</th>\n",
       "      <td>0.535725</td>\n",
       "      <td>0.570036</td>\n",
       "      <td>0.828342</td>\n",
       "      <td>0.898253</td>\n",
       "      <td>0.083417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13 19:00:00</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935882</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.833895</td>\n",
       "      <td>0.091490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29 22:00:00</th>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29 23:00:00</th>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30 00:00:00</th>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30 01:00:00</th>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30 02:00:00</th>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3300 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           MW     80GAW     26WIS     26WOS     dTemp    k  \\\n",
       "datetime                                                                     \n",
       "2020-02-13 15:00:00  0.758238  0.929030  0.896882  0.806007  0.092499  0.0   \n",
       "2020-02-13 16:00:00  0.757611  0.931673  0.902748  0.807846  0.096872  0.0   \n",
       "2020-02-13 17:00:00  0.454964  0.593884  0.826490  0.891204  0.078036  0.0   \n",
       "2020-02-13 18:00:00  0.535725  0.570036  0.828342  0.898253  0.083417  0.0   \n",
       "2020-02-13 19:00:00  1.000000  0.935882  0.924051  0.833895  0.091490  0.0   \n",
       "...                       ...       ...       ...       ...       ...  ...   \n",
       "2020-06-29 22:00:00  0.024898  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2020-06-29 23:00:00  0.024898  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2020-06-30 00:00:00  0.024898  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2020-06-30 01:00:00  0.024898  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2020-06-30 02:00:00  0.024898  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "\n",
       "                            Q  \n",
       "datetime                       \n",
       "2020-02-13 15:00:00  0.608074  \n",
       "2020-02-13 16:00:00  0.639797  \n",
       "2020-02-13 17:00:00  0.327521  \n",
       "2020-02-13 18:00:00  0.336397  \n",
       "2020-02-13 19:00:00  0.606597  \n",
       "...                       ...  \n",
       "2020-06-29 22:00:00  0.000000  \n",
       "2020-06-29 23:00:00  0.000000  \n",
       "2020-06-30 00:00:00  0.000000  \n",
       "2020-06-30 01:00:00  0.000000  \n",
       "2020-06-30 02:00:00  0.000000  \n",
       "\n",
       "[3300 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfval = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in, n_steps_out = 16, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3280, 16, 6)\n"
     ]
    }
   ],
   "source": [
    "X, y = split_sequences(dfval, n_steps_in, n_steps_out)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 16, 6)\n",
      "(280, 16, 6)\n",
      "(3000, 6)\n",
      "(280, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:3000,:]\n",
    "X_test = X[3000:,:]\n",
    "\n",
    "y_train = y[:3000,:]\n",
    "y_test = y[3000:,:]\n",
    "\n",
    "\n",
    "print(X_train.shape) # (5973, 3, 31) = (samples, n_steps_in, n_features)\n",
    "print(X_test.shape) # (5973, 2) = (samples, n_steps_out)\n",
    "\n",
    "print(y_train.shape) # (5973, 3, 31) = (samples, n_steps_in, n_features)\n",
    "print(y_test.shape) # (5973, 2) = (samples, n_steps_out)\n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/1000\n",
      "2400/2400 [==============================] - 2s 798us/step - loss: 0.1399 - val_loss: 0.1069\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10691, saving model to models/modelQ.hdf5\n",
      "Epoch 2/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0810 - val_loss: 0.1283\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.10691\n",
      "Epoch 3/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0725 - val_loss: 0.1290\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10691\n",
      "Epoch 4/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0698 - val_loss: 0.1366\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10691\n",
      "Epoch 5/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0670 - val_loss: 0.1173\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10691\n",
      "Epoch 6/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0650 - val_loss: 0.1265\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10691\n",
      "Epoch 7/1000\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.0630 - val_loss: 0.1205\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10691\n",
      "Epoch 8/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0617 - val_loss: 0.1044\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10691 to 0.10445, saving model to models/modelQ.hdf5\n",
      "Epoch 9/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0607 - val_loss: 0.1209\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10445\n",
      "Epoch 10/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0596 - val_loss: 0.1151\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10445\n",
      "Epoch 11/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0594 - val_loss: 0.1212\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.10445\n",
      "Epoch 12/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0579 - val_loss: 0.1014\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10445 to 0.10138, saving model to models/modelQ.hdf5\n",
      "Epoch 13/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0572 - val_loss: 0.1203\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.10138\n",
      "Epoch 14/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0563 - val_loss: 0.1072\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.10138\n",
      "Epoch 15/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0559 - val_loss: 0.1262\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.10138\n",
      "Epoch 16/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0550 - val_loss: 0.1022\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10138\n",
      "Epoch 17/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0545 - val_loss: 0.1095\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10138\n",
      "Epoch 18/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0542 - val_loss: 0.1033\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10138\n",
      "Epoch 19/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0539 - val_loss: 0.1062\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10138\n",
      "Epoch 20/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0534 - val_loss: 0.1056\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10138\n",
      "Epoch 21/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0537 - val_loss: 0.1105\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10138\n",
      "Epoch 22/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0523 - val_loss: 0.1112\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10138\n",
      "Epoch 23/1000\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.0520 - val_loss: 0.1148\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10138\n",
      "Epoch 24/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0516 - val_loss: 0.1009\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10138 to 0.10094, saving model to models/modelQ.hdf5\n",
      "Epoch 25/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0513 - val_loss: 0.1042\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10094\n",
      "Epoch 26/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0504 - val_loss: 0.1045\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10094\n",
      "Epoch 27/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0503 - val_loss: 0.1179\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10094\n",
      "Epoch 28/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0509 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.10094 to 0.09396, saving model to models/modelQ.hdf5\n",
      "Epoch 29/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0496 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09396\n",
      "Epoch 30/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0502 - val_loss: 0.0995\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09396\n",
      "Epoch 31/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0497 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09396\n",
      "Epoch 32/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0489 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09396 to 0.08971, saving model to models/modelQ.hdf5\n",
      "Epoch 33/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0486 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.08971\n",
      "Epoch 34/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0486 - val_loss: 0.1016\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.08971\n",
      "Epoch 35/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0481 - val_loss: 0.1052\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.08971\n",
      "Epoch 36/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0479 - val_loss: 0.0995\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.08971\n",
      "Epoch 37/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0475 - val_loss: 0.1027\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.08971\n",
      "Epoch 38/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0471 - val_loss: 0.0976\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.08971\n",
      "Epoch 39/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0475 - val_loss: 0.0846\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08971 to 0.08456, saving model to models/modelQ.hdf5\n",
      "Epoch 40/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0479 - val_loss: 0.1019\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.08456\n",
      "Epoch 41/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0473 - val_loss: 0.0943\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.08456\n",
      "Epoch 42/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0466 - val_loss: 0.0996\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.08456\n",
      "Epoch 43/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0470 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.08456\n",
      "Epoch 44/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0463 - val_loss: 0.0991\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.08456\n",
      "Epoch 45/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0460 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.08456\n",
      "Epoch 46/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0462 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.08456\n",
      "Epoch 47/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0463 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.08456\n",
      "Epoch 48/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0456 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.08456\n",
      "Epoch 49/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0456 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.08456\n",
      "Epoch 50/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0456 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.08456\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0451 - val_loss: 0.1009\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.08456\n",
      "Epoch 52/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0452 - val_loss: 0.1011\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.08456\n",
      "Epoch 53/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0449 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.08456\n",
      "Epoch 54/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0445 - val_loss: 0.0982\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.08456\n",
      "Epoch 55/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0449 - val_loss: 0.0936\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.08456\n",
      "Epoch 56/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0459 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.08456 to 0.08140, saving model to models/modelQ.hdf5\n",
      "Epoch 57/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0442 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.08140\n",
      "Epoch 58/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0444 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.08140\n",
      "Epoch 59/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0443 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.08140\n",
      "Epoch 60/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0439 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.08140\n",
      "Epoch 61/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0443 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.08140\n",
      "Epoch 62/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0440 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.08140\n",
      "Epoch 63/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0437 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.08140\n",
      "Epoch 64/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0436 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.08140\n",
      "Epoch 65/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0432 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.08140\n",
      "Epoch 66/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0435 - val_loss: 0.1008\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.08140\n",
      "Epoch 67/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0431 - val_loss: 0.0873\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.08140\n",
      "Epoch 68/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0430 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.08140\n",
      "Epoch 69/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0429 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.08140\n",
      "Epoch 70/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0431 - val_loss: 0.1020\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.08140\n",
      "Epoch 71/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0426 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.08140\n",
      "Epoch 72/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0422 - val_loss: 0.0933\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.08140\n",
      "Epoch 73/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0424 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.08140\n",
      "Epoch 74/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0423 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.08140\n",
      "Epoch 75/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0423 - val_loss: 0.0984\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.08140\n",
      "Epoch 76/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0416 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.08140\n",
      "Epoch 77/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0422 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.08140\n",
      "Epoch 78/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0421 - val_loss: 0.1022\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.08140\n",
      "Epoch 79/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0418 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.08140\n",
      "Epoch 80/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0423 - val_loss: 0.1079\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.08140\n",
      "Epoch 81/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0420 - val_loss: 0.0884\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.08140\n",
      "Epoch 82/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0419 - val_loss: 0.0772\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.08140 to 0.07716, saving model to models/modelQ.hdf5\n",
      "Epoch 83/1000\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.0413 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07716\n",
      "Epoch 84/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0419 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.07716\n",
      "Epoch 85/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0413 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.07716\n",
      "Epoch 86/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0410 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.07716\n",
      "Epoch 87/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0409 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.07716 to 0.07549, saving model to models/modelQ.hdf5\n",
      "Epoch 88/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0410 - val_loss: 0.0779\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.07549\n",
      "Epoch 89/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0409 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.07549\n",
      "Epoch 90/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0405 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.07549\n",
      "Epoch 91/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0405 - val_loss: 0.0749\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.07549 to 0.07494, saving model to models/modelQ.hdf5\n",
      "Epoch 92/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0411 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.07494\n",
      "Epoch 93/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0413 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.07494\n",
      "Epoch 94/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0403 - val_loss: 0.1012\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.07494\n",
      "Epoch 95/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0405 - val_loss: 0.1032\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.07494\n",
      "Epoch 96/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0407 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.07494\n",
      "Epoch 97/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0410 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.07494\n",
      "Epoch 98/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0407 - val_loss: 0.0906\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.07494\n",
      "Epoch 99/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0402 - val_loss: 0.0931\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.07494\n",
      "Epoch 100/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0401 - val_loss: 0.0884\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.07494\n",
      "Epoch 101/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0406 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.07494\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0398 - val_loss: 0.1009\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.07494\n",
      "Epoch 103/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0396 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.07494\n",
      "Epoch 104/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0396 - val_loss: 0.0757\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.07494\n",
      "Epoch 105/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0397 - val_loss: 0.0723\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.07494 to 0.07229, saving model to models/modelQ.hdf5\n",
      "Epoch 106/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0401 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.07229\n",
      "Epoch 107/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0406 - val_loss: 0.0979\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.07229\n",
      "Epoch 108/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0398 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.07229\n",
      "Epoch 109/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0396 - val_loss: 0.1131\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.07229\n",
      "Epoch 110/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0395 - val_loss: 0.0893\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.07229\n",
      "Epoch 111/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0393 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.07229\n",
      "Epoch 112/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0393 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.07229\n",
      "Epoch 113/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0392 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.07229\n",
      "Epoch 114/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0387 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.07229\n",
      "Epoch 115/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0394 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.07229\n",
      "Epoch 116/1000\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.0392 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.07229\n",
      "Epoch 117/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0385 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.07229\n",
      "Epoch 118/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0392 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.07229\n",
      "Epoch 119/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0387 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.07229\n",
      "Epoch 120/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0390 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.07229\n",
      "Epoch 121/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0385 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.07229\n",
      "Epoch 122/1000\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.0381 - val_loss: 0.0767\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.07229\n",
      "Epoch 123/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0385 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.07229\n",
      "Epoch 124/1000\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.0385 - val_loss: 0.0942\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.07229\n",
      "Epoch 125/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0382 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.07229\n",
      "Epoch 126/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0380 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.07229\n",
      "Epoch 127/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0379 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.07229\n",
      "Epoch 128/1000\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.0388 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.07229\n",
      "Epoch 129/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0383 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.07229\n",
      "Epoch 130/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0379 - val_loss: 0.0873\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.07229\n",
      "Epoch 131/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0380 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.07229\n",
      "Epoch 132/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0384 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.07229\n",
      "Epoch 133/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0384 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.07229\n",
      "Epoch 134/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0386 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.07229\n",
      "Epoch 135/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0379 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.07229\n",
      "Epoch 136/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0377 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.07229\n",
      "Epoch 137/1000\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.0387 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.07229\n",
      "Epoch 138/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0381 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.07229\n",
      "Epoch 139/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0376 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.07229\n",
      "Epoch 140/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0373 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.07229\n",
      "Epoch 141/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0373 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.07229\n",
      "Epoch 142/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0372 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.07229\n",
      "Epoch 143/1000\n",
      "2400/2400 [==============================] - 0s 62us/step - loss: 0.0371 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.07229\n",
      "Epoch 144/1000\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.0371 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.07229\n",
      "Epoch 145/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0369 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.07229\n",
      "Epoch 146/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0369 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.07229\n",
      "Epoch 147/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0370 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.07229\n",
      "Epoch 148/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0370 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.07229\n",
      "Epoch 149/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0367 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.07229\n",
      "Epoch 150/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0373 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.07229\n",
      "Epoch 151/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0369 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.07229\n",
      "Epoch 152/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0370 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.07229\n",
      "Epoch 153/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.0371 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.07229\n",
      "Epoch 154/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0372 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.07229\n",
      "Epoch 155/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0368 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.07229\n",
      "Epoch 156/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0366 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.07229\n",
      "Epoch 157/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0366 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.07229\n",
      "Epoch 158/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0368 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.07229\n",
      "Epoch 159/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0361 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.07229\n",
      "Epoch 160/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0362 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.07229\n",
      "Epoch 161/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0363 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.07229\n",
      "Epoch 162/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0364 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.07229\n",
      "Epoch 163/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0361 - val_loss: 0.0906\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.07229\n",
      "Epoch 164/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0363 - val_loss: 0.1016\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.07229\n",
      "Epoch 165/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0363 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.07229\n",
      "Epoch 166/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0364 - val_loss: 0.0753\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.07229\n",
      "Epoch 167/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0362 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.07229\n",
      "Epoch 168/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0355 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.07229\n",
      "Epoch 169/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0360 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.07229\n",
      "Epoch 170/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0355 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.07229\n",
      "Epoch 171/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0362 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.07229\n",
      "Epoch 172/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0359 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.07229\n",
      "Epoch 173/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0358 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.07229\n",
      "Epoch 174/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0355 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.07229\n",
      "Epoch 175/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0355 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.07229\n",
      "Epoch 176/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0354 - val_loss: 0.0927\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.07229\n",
      "Epoch 177/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0352 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.07229\n",
      "Epoch 178/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0356 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.07229\n",
      "Epoch 179/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0354 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.07229\n",
      "Epoch 180/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0361 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.07229\n",
      "Epoch 181/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0358 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.07229\n",
      "Epoch 182/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0352 - val_loss: 0.0992\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.07229\n",
      "Epoch 183/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0357 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.07229\n",
      "Epoch 184/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0354 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.07229\n",
      "Epoch 185/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0353 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.07229\n",
      "Epoch 186/1000\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.0352 - val_loss: 0.0843\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.07229\n",
      "Epoch 187/1000\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.0347 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.07229\n",
      "Epoch 188/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0355 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.07229\n",
      "Epoch 189/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0358 - val_loss: 0.0954\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.07229\n",
      "Epoch 190/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0354 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.07229\n",
      "Epoch 191/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0350 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.07229\n",
      "Epoch 192/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0352 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.07229\n",
      "Epoch 193/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0348 - val_loss: 0.0867\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.07229\n",
      "Epoch 194/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0352 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.07229\n",
      "Epoch 195/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0350 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.07229\n",
      "Epoch 196/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0351 - val_loss: 0.0784\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.07229\n",
      "Epoch 197/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0346 - val_loss: 0.0919\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.07229\n",
      "Epoch 198/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0348 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.07229\n",
      "Epoch 199/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0346 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.07229\n",
      "Epoch 200/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0345 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.07229\n",
      "Epoch 201/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0344 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.07229\n",
      "Epoch 202/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0342 - val_loss: 0.0931\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.07229\n",
      "Epoch 203/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0346 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.07229\n",
      "Epoch 204/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0349 - val_loss: 0.0824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00204: val_loss did not improve from 0.07229\n",
      "Epoch 205/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0346 - val_loss: 0.0899\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.07229\n",
      "Epoch 206/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0349 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.07229\n",
      "Epoch 207/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0340 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.07229\n",
      "Epoch 208/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0344 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.07229\n",
      "Epoch 209/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0347 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.07229\n",
      "Epoch 210/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0341 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.07229\n",
      "Epoch 211/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0345 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.07229\n",
      "Epoch 212/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0344 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.07229\n",
      "Epoch 213/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0342 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.07229\n",
      "Epoch 214/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0338 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.07229\n",
      "Epoch 215/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0340 - val_loss: 0.0739\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.07229\n",
      "Epoch 216/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0335 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.07229\n",
      "Epoch 217/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0339 - val_loss: 0.0958\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.07229\n",
      "Epoch 218/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0351 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.07229\n",
      "Epoch 219/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0337 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.07229\n",
      "Epoch 220/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0334 - val_loss: 0.0858\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.07229\n",
      "Epoch 221/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0336 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.07229\n",
      "Epoch 222/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0332 - val_loss: 0.0750\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.07229\n",
      "Epoch 223/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0334 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.07229\n",
      "Epoch 224/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0333 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.07229\n",
      "Epoch 225/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0335 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.07229\n",
      "Epoch 226/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0333 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.07229\n",
      "Epoch 227/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0334 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.07229\n",
      "Epoch 228/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0335 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.07229\n",
      "Epoch 229/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0333 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.07229\n",
      "Epoch 230/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0337 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.07229\n",
      "Epoch 231/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0332 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.07229\n",
      "Epoch 232/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0335 - val_loss: 0.0899\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.07229\n",
      "Epoch 233/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0330 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.07229\n",
      "Epoch 234/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0332 - val_loss: 0.0764\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.07229\n",
      "Epoch 235/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0327 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.07229\n",
      "Epoch 236/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0331 - val_loss: 0.0958\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.07229\n",
      "Epoch 237/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0332 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.07229\n",
      "Epoch 238/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0330 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.07229\n",
      "Epoch 239/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0327 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.07229\n",
      "Epoch 240/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0330 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.07229\n",
      "Epoch 241/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0331 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.07229\n",
      "Epoch 242/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0335 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.07229\n",
      "Epoch 243/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0332 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.07229\n",
      "Epoch 244/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0327 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.07229\n",
      "Epoch 245/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0325 - val_loss: 0.0741\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.07229\n",
      "Epoch 246/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0326 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.07229\n",
      "Epoch 247/1000\n",
      "2400/2400 [==============================] - 0s 44us/step - loss: 0.0328 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.07229\n",
      "Epoch 248/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0328 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.07229\n",
      "Epoch 249/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0329 - val_loss: 0.0747\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.07229\n",
      "Epoch 250/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0325 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.07229\n",
      "Epoch 251/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0326 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.07229\n",
      "Epoch 252/1000\n",
      "2400/2400 [==============================] - 0s 59us/step - loss: 0.0327 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.07229\n",
      "Epoch 253/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0322 - val_loss: 0.0748\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.07229\n",
      "Epoch 254/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0327 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.07229\n",
      "Epoch 255/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0335 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.07229\n",
      "Epoch 256/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0328 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.07229\n",
      "Epoch 257/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0324 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.07229\n",
      "Epoch 258/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0323 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.07229\n",
      "Epoch 259/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0322 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.07229\n",
      "Epoch 260/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0321 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.07229\n",
      "Epoch 261/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0319 - val_loss: 0.0867\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.07229\n",
      "Epoch 262/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0320 - val_loss: 0.0977\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.07229\n",
      "Epoch 263/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0324 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.07229\n",
      "Epoch 264/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0320 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.07229\n",
      "Epoch 265/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0319 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.07229\n",
      "Epoch 266/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0319 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.07229\n",
      "Epoch 267/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0317 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.07229\n",
      "Epoch 268/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0325 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.07229\n",
      "Epoch 269/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0323 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.07229\n",
      "Epoch 270/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0325 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.07229\n",
      "Epoch 271/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0320 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.07229\n",
      "Epoch 272/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0318 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.07229\n",
      "Epoch 273/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0319 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.07229\n",
      "Epoch 274/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0317 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.07229\n",
      "Epoch 275/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0316 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.07229\n",
      "Epoch 276/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0318 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.07229\n",
      "Epoch 277/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0317 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.07229\n",
      "Epoch 278/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0319 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.07229\n",
      "Epoch 279/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0319 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.07229\n",
      "Epoch 280/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0319 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.07229\n",
      "Epoch 281/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0316 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.07229\n",
      "Epoch 282/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0315 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.07229\n",
      "Epoch 283/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0313 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.07229\n",
      "Epoch 284/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0314 - val_loss: 0.0711\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.07229 to 0.07110, saving model to models/modelQ.hdf5\n",
      "Epoch 285/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0312 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.07110\n",
      "Epoch 286/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0310 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.07110\n",
      "Epoch 287/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0316 - val_loss: 0.0993\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.07110\n",
      "Epoch 288/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0317 - val_loss: 0.0899\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.07110\n",
      "Epoch 289/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0312 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.07110\n",
      "Epoch 290/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0310 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.07110\n",
      "Epoch 291/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0313 - val_loss: 0.1113\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.07110\n",
      "Epoch 292/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0322 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.07110\n",
      "Epoch 293/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0313 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.07110\n",
      "Epoch 294/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0319 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.07110\n",
      "Epoch 295/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0310 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.07110\n",
      "Epoch 296/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0314 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.07110\n",
      "Epoch 297/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0308 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.07110\n",
      "Epoch 298/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0308 - val_loss: 0.0816\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.07110\n",
      "Epoch 299/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0311 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.07110\n",
      "Epoch 300/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0310 - val_loss: 0.0858\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.07110\n",
      "Epoch 301/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0317 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.07110\n",
      "Epoch 302/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0315 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.07110\n",
      "Epoch 303/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0315 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.07110\n",
      "Epoch 304/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0308 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.07110\n",
      "Epoch 305/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0309 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.07110\n",
      "Epoch 306/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0305 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.07110\n",
      "Epoch 307/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0309 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.07110\n",
      "Epoch 308/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0318 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.07110\n",
      "Epoch 309/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0306 - val_loss: 0.0712\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.07110\n",
      "Epoch 310/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0305 - val_loss: 0.0877\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.07110\n",
      "Epoch 311/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0309 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.07110\n",
      "Epoch 312/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0303 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.07110\n",
      "Epoch 313/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0307 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.07110\n",
      "Epoch 314/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0303 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.07110\n",
      "Epoch 315/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0302 - val_loss: 0.0737\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.07110\n",
      "Epoch 316/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0304 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.07110\n",
      "Epoch 317/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0303 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.07110\n",
      "Epoch 318/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0306 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.07110\n",
      "Epoch 319/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0305 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.07110\n",
      "Epoch 320/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0301 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.07110\n",
      "Epoch 321/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0303 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.07110\n",
      "Epoch 322/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0306 - val_loss: 0.0998\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.07110\n",
      "Epoch 323/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0304 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.07110\n",
      "Epoch 324/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0305 - val_loss: 0.0753\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.07110\n",
      "Epoch 325/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0306 - val_loss: 0.0846\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.07110\n",
      "Epoch 326/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0306 - val_loss: 0.0769\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.07110\n",
      "Epoch 327/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0303 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.07110\n",
      "Epoch 328/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0298 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.07110\n",
      "Epoch 329/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0304 - val_loss: 0.0755\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.07110\n",
      "Epoch 330/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0304 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.07110\n",
      "Epoch 331/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0300 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.07110\n",
      "Epoch 332/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0303 - val_loss: 0.0718\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.07110\n",
      "Epoch 333/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0299 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.07110\n",
      "Epoch 334/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0302 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.07110\n",
      "Epoch 335/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0298 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.07110\n",
      "Epoch 336/1000\n",
      "2400/2400 [==============================] - 0s 45us/step - loss: 0.0299 - val_loss: 0.0932\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.07110\n",
      "Epoch 337/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0305 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.07110\n",
      "Epoch 338/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0299 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.07110\n",
      "Epoch 339/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0301 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.07110\n",
      "Epoch 340/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0301 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.07110\n",
      "Epoch 341/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0297 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.07110\n",
      "Epoch 342/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0298 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.07110\n",
      "Epoch 343/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0297 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.07110\n",
      "Epoch 344/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0297 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.07110\n",
      "Epoch 345/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0297 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.07110\n",
      "Epoch 346/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0303 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.07110\n",
      "Epoch 347/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0300 - val_loss: 0.0843\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.07110\n",
      "Epoch 348/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0293 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.07110\n",
      "Epoch 349/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0298 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.07110\n",
      "Epoch 350/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0294 - val_loss: 0.0729\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.07110\n",
      "Epoch 351/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0296 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.07110\n",
      "Epoch 352/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0303 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.07110\n",
      "Epoch 353/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0298 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.07110\n",
      "Epoch 354/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0293 - val_loss: 0.0968\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.07110\n",
      "Epoch 355/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0298 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.07110\n",
      "Epoch 356/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0292 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.07110\n",
      "Epoch 357/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0298 - val_loss: 0.0713\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.07110\n",
      "Epoch 358/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0296 - val_loss: 0.0799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00358: val_loss did not improve from 0.07110\n",
      "Epoch 359/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0299 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.07110\n",
      "Epoch 360/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0292 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.07110\n",
      "Epoch 361/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0294 - val_loss: 0.0775\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.07110\n",
      "Epoch 362/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0294 - val_loss: 0.0848\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.07110\n",
      "Epoch 363/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0290 - val_loss: 0.0716\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.07110\n",
      "Epoch 364/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0294 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.07110\n",
      "Epoch 365/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0291 - val_loss: 0.0758\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.07110\n",
      "Epoch 366/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0296 - val_loss: 0.0884\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.07110\n",
      "Epoch 367/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0291 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.07110\n",
      "Epoch 368/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0289 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.07110\n",
      "Epoch 369/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0288 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.07110\n",
      "Epoch 370/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0291 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.07110\n",
      "Epoch 371/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0291 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.07110\n",
      "Epoch 372/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0296 - val_loss: 0.0846\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.07110\n",
      "Epoch 373/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0289 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.07110\n",
      "Epoch 374/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0287 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.07110\n",
      "Epoch 375/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0290 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.07110\n",
      "Epoch 376/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0293 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.07110\n",
      "Epoch 377/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0285 - val_loss: 0.0913\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.07110\n",
      "Epoch 378/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0289 - val_loss: 0.0740\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.07110\n",
      "Epoch 379/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0286 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.07110\n",
      "Epoch 380/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0286 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.07110\n",
      "Epoch 381/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0288 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.07110\n",
      "Epoch 382/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0285 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.07110\n",
      "Epoch 383/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0289 - val_loss: 0.0954\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.07110\n",
      "Epoch 384/1000\n",
      "2400/2400 [==============================] - 0s 48us/step - loss: 0.0292 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.07110\n",
      "Epoch 385/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0288 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.07110\n",
      "Epoch 386/1000\n",
      "2400/2400 [==============================] - 0s 46us/step - loss: 0.0291 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.07110\n",
      "Epoch 387/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0294 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.07110\n",
      "Epoch 388/1000\n",
      "2400/2400 [==============================] - 0s 47us/step - loss: 0.0288 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.07110\n",
      "Epoch 389/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0286 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.07110\n",
      "Epoch 390/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0285 - val_loss: 0.1021\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.07110\n",
      "Epoch 391/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0289 - val_loss: 0.0802\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.07110\n",
      "Epoch 392/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0286 - val_loss: 0.0906\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.07110\n",
      "Epoch 393/1000\n",
      "2400/2400 [==============================] - 0s 49us/step - loss: 0.0286 - val_loss: 0.0908\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.07110\n",
      "Epoch 394/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0284 - val_loss: 0.0906\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.07110\n",
      "Epoch 395/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0296 - val_loss: 0.0838\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.07110\n",
      "Epoch 396/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0283 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.07110\n",
      "Epoch 397/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0285 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.07110\n",
      "Epoch 398/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0285 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.07110\n",
      "Epoch 399/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0290 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.07110\n",
      "Epoch 400/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0282 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.07110\n",
      "Epoch 401/1000\n",
      "2400/2400 [==============================] - 0s 50us/step - loss: 0.0283 - val_loss: 0.0846\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.07110\n",
      "Epoch 402/1000\n",
      "2400/2400 [==============================] - 0s 51us/step - loss: 0.0285 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.07110\n",
      "Epoch 403/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0291 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.07110\n",
      "Epoch 404/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0282 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.07110\n",
      "Epoch 405/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0282 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.07110\n",
      "Epoch 406/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0290 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.07110\n",
      "Epoch 407/1000\n",
      "2400/2400 [==============================] - ETA: 0s - loss: 0.027 - 0s 53us/step - loss: 0.0277 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.07110\n",
      "Epoch 408/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0279 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.07110\n",
      "Epoch 409/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0279 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.07110\n",
      "Epoch 410/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0281 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.07110\n",
      "Epoch 411/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0282 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.07110\n",
      "Epoch 412/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0281 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.07110\n",
      "Epoch 413/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0279 - val_loss: 0.0931\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.07110\n",
      "Epoch 414/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0286 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.07110\n",
      "Epoch 415/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0282 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.07110\n",
      "Epoch 416/1000\n",
      "2400/2400 [==============================] - 0s 52us/step - loss: 0.0275 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.07110\n",
      "Epoch 417/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0284 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.07110\n",
      "Epoch 418/1000\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.0278 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.07110\n",
      "Epoch 419/1000\n",
      "2400/2400 [==============================] - 0s 53us/step - loss: 0.0287 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.07110\n",
      "Epoch 420/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0281 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.07110\n",
      "Epoch 421/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0287 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.07110\n",
      "Epoch 422/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0278 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.07110\n",
      "Epoch 423/1000\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.0276 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.07110\n",
      "Epoch 424/1000\n",
      "2400/2400 [==============================] - 0s 54us/step - loss: 0.0283 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.07110\n",
      "Epoch 425/1000\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.0284 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.07110\n",
      "Epoch 426/1000\n",
      "2400/2400 [==============================] - 0s 55us/step - loss: 0.0284 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.07110\n",
      "Epoch 427/1000\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.0280 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.07110\n",
      "Epoch 428/1000\n",
      "2400/2400 [==============================] - 0s 56us/step - loss: 0.0277 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.07110\n",
      "Epoch 429/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0278 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.07110\n",
      "Epoch 430/1000\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.0277 - val_loss: 0.0875\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.07110\n",
      "Epoch 431/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0273 - val_loss: 0.0877\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.07110\n",
      "Epoch 432/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0276 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.07110\n",
      "Epoch 433/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0277 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.07110\n",
      "Epoch 434/1000\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.0277 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.07110\n",
      "Epoch 435/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0276 - val_loss: 0.1006\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.07110\n",
      "Epoch 436/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0281 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.07110\n",
      "Epoch 437/1000\n",
      "2400/2400 [==============================] - 0s 58us/step - loss: 0.0278 - val_loss: 0.0752\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.07110\n",
      "Epoch 438/1000\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.0280 - val_loss: 0.0778\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.07110\n",
      "Epoch 439/1000\n",
      "2400/2400 [==============================] - 0s 57us/step - loss: 0.0273 - val_loss: 0.0902\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.07110\n",
      "Epoch 440/1000\n",
      "2400/2400 [==============================] - 0s 59us/step - loss: 0.0270 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.07110\n",
      "Epoch 441/1000\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.0276 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.07110\n",
      "Epoch 442/1000\n",
      "2400/2400 [==============================] - 0s 59us/step - loss: 0.0272 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.07110\n",
      "Epoch 443/1000\n",
      "2400/2400 [==============================] - 0s 60us/step - loss: 0.0275 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.07110\n",
      "Epoch 444/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0277 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.07110\n",
      "Epoch 445/1000\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.0277 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.07110\n",
      "Epoch 446/1000\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.0272 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.07110\n",
      "Epoch 447/1000\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.0276 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.07110\n",
      "Epoch 448/1000\n",
      "2400/2400 [==============================] - 0s 61us/step - loss: 0.0280 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.07110\n",
      "Epoch 449/1000\n",
      "2400/2400 [==============================] - 0s 62us/step - loss: 0.0272 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.07110\n",
      "Epoch 450/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0270 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.07110\n",
      "Epoch 451/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0274 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.07110\n",
      "Epoch 452/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0271 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.07110\n",
      "Epoch 453/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0272 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.07110\n",
      "Epoch 454/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0273 - val_loss: 0.0811\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.07110\n",
      "Epoch 455/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0270 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.07110\n",
      "Epoch 456/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0274 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.07110\n",
      "Epoch 457/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0275 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.07110\n",
      "Epoch 458/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0273 - val_loss: 0.0893\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.07110\n",
      "Epoch 459/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0269 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.07110\n",
      "Epoch 460/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0269 - val_loss: 0.0971\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.07110\n",
      "Epoch 461/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0271 - val_loss: 0.0869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00461: val_loss did not improve from 0.07110\n",
      "Epoch 462/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0271 - val_loss: 0.0809\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.07110\n",
      "Epoch 463/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0271 - val_loss: 0.0746\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.07110\n",
      "Epoch 464/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0272 - val_loss: 0.0874\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.07110\n",
      "Epoch 465/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0275 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.07110\n",
      "Epoch 466/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0268 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.07110\n",
      "Epoch 467/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0267 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.07110\n",
      "Epoch 468/1000\n",
      "2400/2400 [==============================] - 0s 87us/step - loss: 0.0266 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.07110\n",
      "Epoch 469/1000\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.0273 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.07110\n",
      "Epoch 470/1000\n",
      "2400/2400 [==============================] - 0s 92us/step - loss: 0.0272 - val_loss: 0.0992\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.07110\n",
      "Epoch 471/1000\n",
      "2400/2400 [==============================] - 0s 86us/step - loss: 0.0268 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.07110\n",
      "Epoch 472/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0272 - val_loss: 0.0867\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.07110\n",
      "Epoch 473/1000\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.0265 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.07110\n",
      "Epoch 474/1000\n",
      "2400/2400 [==============================] - 0s 105us/step - loss: 0.0267 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.07110\n",
      "Epoch 475/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0268 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.07110\n",
      "Epoch 476/1000\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.0267 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.07110\n",
      "Epoch 477/1000\n",
      "2400/2400 [==============================] - 0s 86us/step - loss: 0.0264 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.07110\n",
      "Epoch 478/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0270 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.07110\n",
      "Epoch 479/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0267 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.07110\n",
      "Epoch 480/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0265 - val_loss: 0.0761\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.07110\n",
      "Epoch 481/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0274 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.07110\n",
      "Epoch 482/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0266 - val_loss: 0.0919\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.07110\n",
      "Epoch 483/1000\n",
      "2400/2400 [==============================] - 0s 88us/step - loss: 0.0272 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.07110\n",
      "Epoch 484/1000\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.0269 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.07110\n",
      "Epoch 485/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0278 - val_loss: 0.0787\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.07110\n",
      "Epoch 486/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0265 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.07110\n",
      "Epoch 487/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0268 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.07110\n",
      "Epoch 488/1000\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.0265 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.07110\n",
      "Epoch 489/1000\n",
      "2400/2400 [==============================] - 0s 86us/step - loss: 0.0263 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.07110\n",
      "Epoch 490/1000\n",
      "2400/2400 [==============================] - 0s 93us/step - loss: 0.0266 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.07110\n",
      "Epoch 491/1000\n",
      "2400/2400 [==============================] - 0s 92us/step - loss: 0.0270 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.07110\n",
      "Epoch 492/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0264 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.07110\n",
      "Epoch 493/1000\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.0262 - val_loss: 0.0930\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.07110\n",
      "Epoch 494/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0262 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.07110\n",
      "Epoch 495/1000\n",
      "2400/2400 [==============================] - 0s 97us/step - loss: 0.0262 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.07110\n",
      "Epoch 496/1000\n",
      "2400/2400 [==============================] - 0s 88us/step - loss: 0.0262 - val_loss: 0.0776\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.07110\n",
      "Epoch 497/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0261 - val_loss: 0.0960\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.07110\n",
      "Epoch 498/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0265 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.07110\n",
      "Epoch 499/1000\n",
      "2400/2400 [==============================] - 0s 96us/step - loss: 0.0272 - val_loss: 0.0816\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.07110\n",
      "Epoch 500/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0264 - val_loss: 0.0962\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.07110\n",
      "Epoch 501/1000\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.0265 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.07110\n",
      "Epoch 502/1000\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.0266 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.07110\n",
      "Epoch 503/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0264 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.07110\n",
      "Epoch 504/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0257 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.07110\n",
      "Epoch 505/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0260 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.07110\n",
      "Epoch 506/1000\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.0265 - val_loss: 0.0978\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.07110\n",
      "Epoch 507/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0263 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.07110\n",
      "Epoch 508/1000\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.0259 - val_loss: 0.0961\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.07110\n",
      "Epoch 509/1000\n",
      "2400/2400 [==============================] - 0s 84us/step - loss: 0.0261 - val_loss: 0.0927\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.07110\n",
      "Epoch 510/1000\n",
      "2400/2400 [==============================] - 0s 86us/step - loss: 0.0260 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.07110\n",
      "Epoch 511/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0259 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.07110\n",
      "Epoch 512/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0258 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.07110\n",
      "Epoch 513/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0260 - val_loss: 0.0946\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.07110\n",
      "Epoch 514/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0264 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.07110\n",
      "Epoch 515/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0260 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.07110\n",
      "Epoch 516/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0262 - val_loss: 0.0845\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.07110\n",
      "Epoch 517/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0257 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.07110\n",
      "Epoch 518/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0261 - val_loss: 0.0797\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.07110\n",
      "Epoch 519/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0258 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.07110\n",
      "Epoch 520/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0261 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.07110\n",
      "Epoch 521/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0259 - val_loss: 0.0781\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.07110\n",
      "Epoch 522/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0266 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.07110\n",
      "Epoch 523/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0259 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.07110\n",
      "Epoch 524/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0254 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.07110\n",
      "Epoch 525/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0259 - val_loss: 0.0829\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.07110\n",
      "Epoch 526/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0257 - val_loss: 0.0867\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.07110\n",
      "Epoch 527/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0260 - val_loss: 0.1011\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.07110\n",
      "Epoch 528/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0258 - val_loss: 0.0953\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.07110\n",
      "Epoch 529/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0260 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.07110\n",
      "Epoch 530/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0258 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.07110\n",
      "Epoch 531/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0260 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.07110\n",
      "Epoch 532/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0255 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.07110\n",
      "Epoch 533/1000\n",
      "2400/2400 [==============================] - 0s 87us/step - loss: 0.0257 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.07110\n",
      "Epoch 534/1000\n",
      "2400/2400 [==============================] - 0s 108us/step - loss: 0.0262 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.07110\n",
      "Epoch 535/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0263 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.07110\n",
      "Epoch 536/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0258 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.07110\n",
      "Epoch 537/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0255 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.07110\n",
      "Epoch 538/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0258 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.07110\n",
      "Epoch 539/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0256 - val_loss: 0.0828\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.07110\n",
      "Epoch 540/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0256 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.07110\n",
      "Epoch 541/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0256 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.07110\n",
      "Epoch 542/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0262 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.07110\n",
      "Epoch 543/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0261 - val_loss: 0.0997\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.07110\n",
      "Epoch 544/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0262 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.07110\n",
      "Epoch 545/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0253 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.07110\n",
      "Epoch 546/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0250 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.07110\n",
      "Epoch 547/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0259 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.07110\n",
      "Epoch 548/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0255 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.07110\n",
      "Epoch 549/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0255 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.07110\n",
      "Epoch 550/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0250 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.07110\n",
      "Epoch 551/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0254 - val_loss: 0.0996\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.07110\n",
      "Epoch 552/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0252 - val_loss: 0.0863\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.07110\n",
      "Epoch 553/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0252 - val_loss: 0.0822\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.07110\n",
      "Epoch 554/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0251 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.07110\n",
      "Epoch 555/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0253 - val_loss: 0.0848\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.07110\n",
      "Epoch 556/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0253 - val_loss: 0.0882\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.07110\n",
      "Epoch 557/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0254 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.07110\n",
      "Epoch 558/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0256 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.07110\n",
      "Epoch 559/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0254 - val_loss: 0.0980\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.07110\n",
      "Epoch 560/1000\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.0254 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.07110\n",
      "Epoch 561/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0254 - val_loss: 0.0920\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.07110\n",
      "Epoch 562/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0254 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.07110\n",
      "Epoch 563/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0253 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.07110\n",
      "Epoch 564/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0249 - val_loss: 0.0841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00564: val_loss did not improve from 0.07110\n",
      "Epoch 565/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0247 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.07110\n",
      "Epoch 566/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0248 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.07110\n",
      "Epoch 567/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0249 - val_loss: 0.0806\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.07110\n",
      "Epoch 568/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0254 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.07110\n",
      "Epoch 569/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0250 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.07110\n",
      "Epoch 570/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0250 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.07110\n",
      "Epoch 571/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0251 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.07110\n",
      "Epoch 572/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0250 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.07110\n",
      "Epoch 573/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0251 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.07110\n",
      "Epoch 574/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0248 - val_loss: 0.0843\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.07110\n",
      "Epoch 575/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0252 - val_loss: 0.0902\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.07110\n",
      "Epoch 576/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0247 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.07110\n",
      "Epoch 577/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0249 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.07110\n",
      "Epoch 578/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0257 - val_loss: 0.0971\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.07110\n",
      "Epoch 579/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0252 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.07110\n",
      "Epoch 580/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0248 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.07110\n",
      "Epoch 581/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0246 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.07110\n",
      "Epoch 582/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0245 - val_loss: 0.1015\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.07110\n",
      "Epoch 583/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0254 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.07110\n",
      "Epoch 584/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0249 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.07110\n",
      "Epoch 585/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0250 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.07110\n",
      "Epoch 586/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0251 - val_loss: 0.0937\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.07110\n",
      "Epoch 587/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0246 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.07110\n",
      "Epoch 588/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0247 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.07110\n",
      "Epoch 589/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0247 - val_loss: 0.0773\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.07110\n",
      "Epoch 590/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0252 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.07110\n",
      "Epoch 591/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0250 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.07110\n",
      "Epoch 592/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0248 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.07110\n",
      "Epoch 593/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0242 - val_loss: 0.0802\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.07110\n",
      "Epoch 594/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0245 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.07110\n",
      "Epoch 595/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0245 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.07110\n",
      "Epoch 596/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0247 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.07110\n",
      "Epoch 597/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0244 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.07110\n",
      "Epoch 598/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0246 - val_loss: 0.0873\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.07110\n",
      "Epoch 599/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0246 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.07110\n",
      "Epoch 600/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0244 - val_loss: 0.0765\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.07110\n",
      "Epoch 601/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0244 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.07110\n",
      "Epoch 602/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0245 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.07110\n",
      "Epoch 603/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0246 - val_loss: 0.0827\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.07110\n",
      "Epoch 604/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0244 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.07110\n",
      "Epoch 605/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0242 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.07110\n",
      "Epoch 606/1000\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.0243 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.07110\n",
      "Epoch 607/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0244 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.07110\n",
      "Epoch 608/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0242 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.07110\n",
      "Epoch 609/1000\n",
      "2400/2400 [==============================] - 0s 84us/step - loss: 0.0243 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.07110\n",
      "Epoch 610/1000\n",
      "2400/2400 [==============================] - 0s 87us/step - loss: 0.0241 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.07110\n",
      "Epoch 611/1000\n",
      "2400/2400 [==============================] - 0s 89us/step - loss: 0.0242 - val_loss: 0.0712\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.07110\n",
      "Epoch 612/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0246 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.07110\n",
      "Epoch 613/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0243 - val_loss: 0.0857\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.07110\n",
      "Epoch 614/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0243 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.07110\n",
      "Epoch 615/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0240 - val_loss: 0.0930\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.07110\n",
      "Epoch 616/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0246 - val_loss: 0.0788\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.07110\n",
      "Epoch 617/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0245 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.07110\n",
      "Epoch 618/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0240 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.07110\n",
      "Epoch 619/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0243 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.07110\n",
      "Epoch 620/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0243 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.07110\n",
      "Epoch 621/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0242 - val_loss: 0.0786\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.07110\n",
      "Epoch 622/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0241 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.07110\n",
      "Epoch 623/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0243 - val_loss: 0.0907\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.07110\n",
      "Epoch 624/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0244 - val_loss: 0.0855\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.07110\n",
      "Epoch 625/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0244 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.07110\n",
      "Epoch 626/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0243 - val_loss: 0.0791\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.07110\n",
      "Epoch 627/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0240 - val_loss: 0.0837\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.07110\n",
      "Epoch 628/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0239 - val_loss: 0.0835\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.07110\n",
      "Epoch 629/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0244 - val_loss: 0.0794\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.07110\n",
      "Epoch 630/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0240 - val_loss: 0.0855\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.07110\n",
      "Epoch 631/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0240 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.07110\n",
      "Epoch 632/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0242 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.07110\n",
      "Epoch 633/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0238 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.07110\n",
      "Epoch 634/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0245 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.07110\n",
      "Epoch 635/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0246 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.07110\n",
      "Epoch 636/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0244 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.07110\n",
      "Epoch 637/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0238 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.07110\n",
      "Epoch 638/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0239 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.07110\n",
      "Epoch 639/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0243 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.07110\n",
      "Epoch 640/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0241 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.07110\n",
      "Epoch 641/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0241 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.07110\n",
      "Epoch 642/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0241 - val_loss: 0.0941\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.07110\n",
      "Epoch 643/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0239 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.07110\n",
      "Epoch 644/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0238 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.07110\n",
      "Epoch 645/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0236 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.07110\n",
      "Epoch 646/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0234 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.07110\n",
      "Epoch 647/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0239 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.07110\n",
      "Epoch 648/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0241 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.07110\n",
      "Epoch 649/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0237 - val_loss: 0.0805\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.07110\n",
      "Epoch 650/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0236 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.07110\n",
      "Epoch 651/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0237 - val_loss: 0.0989\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.07110\n",
      "Epoch 652/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0237 - val_loss: 0.0918\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.07110\n",
      "Epoch 653/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0239 - val_loss: 0.0975\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.07110\n",
      "Epoch 654/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0235 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.07110\n",
      "Epoch 655/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0235 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.07110\n",
      "Epoch 656/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0237 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.07110\n",
      "Epoch 657/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0236 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.07110\n",
      "Epoch 658/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0235 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.07110\n",
      "Epoch 659/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0236 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.07110\n",
      "Epoch 660/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0233 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.07110\n",
      "Epoch 661/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0235 - val_loss: 0.0996\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.07110\n",
      "Epoch 662/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0232 - val_loss: 0.0824\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.07110\n",
      "Epoch 663/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0243 - val_loss: 0.0988\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.07110\n",
      "Epoch 664/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0242 - val_loss: 0.0856\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.07110\n",
      "Epoch 665/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0239 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.07110\n",
      "Epoch 666/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0239 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.07110\n",
      "Epoch 667/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0235 - val_loss: 0.0873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00667: val_loss did not improve from 0.07110\n",
      "Epoch 668/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0235 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.07110\n",
      "Epoch 669/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0236 - val_loss: 0.0789\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.07110\n",
      "Epoch 670/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0238 - val_loss: 0.0927\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.07110\n",
      "Epoch 671/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0233 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.07110\n",
      "Epoch 672/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0235 - val_loss: 0.0917\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.07110\n",
      "Epoch 673/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0237 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.07110\n",
      "Epoch 674/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0236 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.07110\n",
      "Epoch 675/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0233 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.07110\n",
      "Epoch 676/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0237 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.07110\n",
      "Epoch 677/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0241 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.07110\n",
      "Epoch 678/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0232 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.07110\n",
      "Epoch 679/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0233 - val_loss: 0.0864\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.07110\n",
      "Epoch 680/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0233 - val_loss: 0.0998\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.07110\n",
      "Epoch 681/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0233 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.07110\n",
      "Epoch 682/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0235 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.07110\n",
      "Epoch 683/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0236 - val_loss: 0.0933\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.07110\n",
      "Epoch 684/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0235 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.07110\n",
      "Epoch 685/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0234 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.07110\n",
      "Epoch 686/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0240 - val_loss: 0.0986\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.07110\n",
      "Epoch 687/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0240 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.07110\n",
      "Epoch 688/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0232 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.07110\n",
      "Epoch 689/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0231 - val_loss: 0.1007\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.07110\n",
      "Epoch 690/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0236 - val_loss: 0.0792\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.07110\n",
      "Epoch 691/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0231 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.07110\n",
      "Epoch 692/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0233 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.07110\n",
      "Epoch 693/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0230 - val_loss: 0.0801\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.07110\n",
      "Epoch 694/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0228 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.07110\n",
      "Epoch 695/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0230 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.07110\n",
      "Epoch 696/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0232 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.07110\n",
      "Epoch 697/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0228 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.07110\n",
      "Epoch 698/1000\n",
      "2400/2400 [==============================] - 0s 86us/step - loss: 0.0233 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.07110\n",
      "Epoch 699/1000\n",
      "2400/2400 [==============================] - 0s 84us/step - loss: 0.0230 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.07110\n",
      "Epoch 700/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0234 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.07110\n",
      "Epoch 701/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0233 - val_loss: 0.0816\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.07110\n",
      "Epoch 702/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0233 - val_loss: 0.0804\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.07110\n",
      "Epoch 703/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0231 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.07110\n",
      "Epoch 704/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0228 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.07110\n",
      "Epoch 705/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0232 - val_loss: 0.0967\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.07110\n",
      "Epoch 706/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0232 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.07110\n",
      "Epoch 707/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0232 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.07110\n",
      "Epoch 708/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0231 - val_loss: 0.0850\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.07110\n",
      "Epoch 709/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0233 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.07110\n",
      "Epoch 710/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0226 - val_loss: 0.0875\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.07110\n",
      "Epoch 711/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0231 - val_loss: 0.0913\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.07110\n",
      "Epoch 712/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0229 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.07110\n",
      "Epoch 713/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0227 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.07110\n",
      "Epoch 714/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0228 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.07110\n",
      "Epoch 715/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0233 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.07110\n",
      "Epoch 716/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0226 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.07110\n",
      "Epoch 717/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0226 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.07110\n",
      "Epoch 718/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0224 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.07110\n",
      "Epoch 719/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0228 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.07110\n",
      "Epoch 720/1000\n",
      "2400/2400 [==============================] - 0s 86us/step - loss: 0.0231 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.07110\n",
      "Epoch 721/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0229 - val_loss: 0.0874\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.07110\n",
      "Epoch 722/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0230 - val_loss: 0.0902\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.07110\n",
      "Epoch 723/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0226 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.07110\n",
      "Epoch 724/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0230 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.07110\n",
      "Epoch 725/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0226 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.07110\n",
      "Epoch 726/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0230 - val_loss: 0.0953\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.07110\n",
      "Epoch 727/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0225 - val_loss: 0.1016\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.07110\n",
      "Epoch 728/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0226 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.07110\n",
      "Epoch 729/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0227 - val_loss: 0.1000\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.07110\n",
      "Epoch 730/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0232 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.07110\n",
      "Epoch 731/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0228 - val_loss: 0.0852\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.07110\n",
      "Epoch 732/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0228 - val_loss: 0.0859\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.07110\n",
      "Epoch 733/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0225 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.07110\n",
      "Epoch 734/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0228 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.07110\n",
      "Epoch 735/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0226 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.07110\n",
      "Epoch 736/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0230 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.07110\n",
      "Epoch 737/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0226 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.07110\n",
      "Epoch 738/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0228 - val_loss: 0.0851\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.07110\n",
      "Epoch 739/1000\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.0228 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.07110\n",
      "Epoch 740/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0226 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.07110\n",
      "Epoch 741/1000\n",
      "2400/2400 [==============================] - 0s 99us/step - loss: 0.0226 - val_loss: 0.1044\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.07110\n",
      "Epoch 742/1000\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.0228 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.07110\n",
      "Epoch 743/1000\n",
      "2400/2400 [==============================] - 0s 93us/step - loss: 0.0227 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.07110\n",
      "Epoch 744/1000\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.0227 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.07110\n",
      "Epoch 745/1000\n",
      "2400/2400 [==============================] - 0s 103us/step - loss: 0.0225 - val_loss: 0.0841\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.07110\n",
      "Epoch 746/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0227 - val_loss: 0.1001\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.07110\n",
      "Epoch 747/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0222 - val_loss: 0.0906\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.07110\n",
      "Epoch 748/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0226 - val_loss: 0.0803\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.07110\n",
      "Epoch 749/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0226 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.07110\n",
      "Epoch 750/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0223 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.07110\n",
      "Epoch 751/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0224 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.07110\n",
      "Epoch 752/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0223 - val_loss: 0.0860\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.07110\n",
      "Epoch 753/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0223 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.07110\n",
      "Epoch 754/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0224 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.07110\n",
      "Epoch 755/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0227 - val_loss: 0.0900\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.07110\n",
      "Epoch 756/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0228 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.07110\n",
      "Epoch 757/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0228 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.07110\n",
      "Epoch 758/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0219 - val_loss: 0.0936\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.07110\n",
      "Epoch 759/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0224 - val_loss: 0.0785\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.07110\n",
      "Epoch 760/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0229 - val_loss: 0.1031\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.07110\n",
      "Epoch 761/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0230 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.07110\n",
      "Epoch 762/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0222 - val_loss: 0.0831\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.07110\n",
      "Epoch 763/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0220 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.07110\n",
      "Epoch 764/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0220 - val_loss: 0.0882\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.07110\n",
      "Epoch 765/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0220 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.07110\n",
      "Epoch 766/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0223 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.07110\n",
      "Epoch 767/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0227 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.07110\n",
      "Epoch 768/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0226 - val_loss: 0.0839\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.07110\n",
      "Epoch 769/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0223 - val_loss: 0.0957\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.07110\n",
      "Epoch 770/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0221 - val_loss: 0.0881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00770: val_loss did not improve from 0.07110\n",
      "Epoch 771/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0223 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.07110\n",
      "Epoch 772/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0226 - val_loss: 0.0946\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.07110\n",
      "Epoch 773/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0223 - val_loss: 0.0919\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.07110\n",
      "Epoch 774/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0220 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.07110\n",
      "Epoch 775/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0222 - val_loss: 0.0825\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.07110\n",
      "Epoch 776/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0225 - val_loss: 0.0947\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.07110\n",
      "Epoch 777/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0226 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.07110\n",
      "Epoch 778/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0224 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.07110\n",
      "Epoch 779/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0224 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.07110\n",
      "Epoch 780/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0224 - val_loss: 0.0813\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.07110\n",
      "Epoch 781/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0229 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.07110\n",
      "Epoch 782/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0218 - val_loss: 0.0814\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.07110\n",
      "Epoch 783/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0221 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.07110\n",
      "Epoch 784/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0217 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.07110\n",
      "Epoch 785/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0220 - val_loss: 0.0832\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.07110\n",
      "Epoch 786/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0225 - val_loss: 0.0873\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.07110\n",
      "Epoch 787/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0220 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.07110\n",
      "Epoch 788/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0221 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.07110\n",
      "Epoch 789/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0222 - val_loss: 0.1034\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.07110\n",
      "Epoch 790/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0222 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.07110\n",
      "Epoch 791/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0219 - val_loss: 0.0918\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.07110\n",
      "Epoch 792/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0219 - val_loss: 0.0979\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.07110\n",
      "Epoch 793/1000\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.0221 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.07110\n",
      "Epoch 794/1000\n",
      "2400/2400 [==============================] - 0s 88us/step - loss: 0.0220 - val_loss: 0.0882\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.07110\n",
      "Epoch 795/1000\n",
      "2400/2400 [==============================] - 0s 84us/step - loss: 0.0216 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.07110\n",
      "Epoch 796/1000\n",
      "2400/2400 [==============================] - 0s 88us/step - loss: 0.0218 - val_loss: 0.1017\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.07110\n",
      "Epoch 797/1000\n",
      "2400/2400 [==============================] - 0s 89us/step - loss: 0.0217 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.07110\n",
      "Epoch 798/1000\n",
      "2400/2400 [==============================] - 0s 110us/step - loss: 0.0215 - val_loss: 0.0967\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.07110\n",
      "Epoch 799/1000\n",
      "2400/2400 [==============================] - 0s 98us/step - loss: 0.0221 - val_loss: 0.0986\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.07110\n",
      "Epoch 800/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0225 - val_loss: 0.0875\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.07110\n",
      "Epoch 801/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0215 - val_loss: 0.0906\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.07110\n",
      "Epoch 802/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0214 - val_loss: 0.0999\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.07110\n",
      "Epoch 803/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0222 - val_loss: 0.0932\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.07110\n",
      "Epoch 804/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0222 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.07110\n",
      "Epoch 805/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0219 - val_loss: 0.0886\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.07110\n",
      "Epoch 806/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0218 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.07110\n",
      "Epoch 807/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0216 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.07110\n",
      "Epoch 808/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0217 - val_loss: 0.0823\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.07110\n",
      "Epoch 809/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0219 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.07110\n",
      "Epoch 810/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0217 - val_loss: 0.0878\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.07110\n",
      "Epoch 811/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0219 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.07110\n",
      "Epoch 812/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0216 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.07110\n",
      "Epoch 813/1000\n",
      "2400/2400 [==============================] - 0s 89us/step - loss: 0.0215 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.07110\n",
      "Epoch 814/1000\n",
      "2400/2400 [==============================] - 0s 87us/step - loss: 0.0214 - val_loss: 0.0855\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.07110\n",
      "Epoch 815/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0218 - val_loss: 0.0881\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.07110\n",
      "Epoch 816/1000\n",
      "2400/2400 [==============================] - 0s 97us/step - loss: 0.0216 - val_loss: 0.0908\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.07110\n",
      "Epoch 817/1000\n",
      "2400/2400 [==============================] - 0s 116us/step - loss: 0.0217 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.07110\n",
      "Epoch 818/1000\n",
      "2400/2400 [==============================] - 0s 124us/step - loss: 0.0222 - val_loss: 0.0902\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.07110\n",
      "Epoch 819/1000\n",
      "2400/2400 [==============================] - 0s 124us/step - loss: 0.0219 - val_loss: 0.0953\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.07110\n",
      "Epoch 820/1000\n",
      "2400/2400 [==============================] - 0s 116us/step - loss: 0.0217 - val_loss: 0.0883\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.07110\n",
      "Epoch 821/1000\n",
      "2400/2400 [==============================] - 0s 125us/step - loss: 0.0215 - val_loss: 0.0889\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.07110\n",
      "Epoch 822/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 125us/step - loss: 0.0216 - val_loss: 0.0939\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.07110\n",
      "Epoch 823/1000\n",
      "2400/2400 [==============================] - 0s 110us/step - loss: 0.0215 - val_loss: 0.1067\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.07110\n",
      "Epoch 824/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0217 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.07110\n",
      "Epoch 825/1000\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.0216 - val_loss: 0.0912\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.07110\n",
      "Epoch 826/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0214 - val_loss: 0.0877\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.07110\n",
      "Epoch 827/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0214 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.07110\n",
      "Epoch 828/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0212 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.07110\n",
      "Epoch 829/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0217 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.07110\n",
      "Epoch 830/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0214 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.07110\n",
      "Epoch 831/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0216 - val_loss: 0.0954\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.07110\n",
      "Epoch 832/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0213 - val_loss: 0.0795\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.07110\n",
      "Epoch 833/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0217 - val_loss: 0.0834\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.07110\n",
      "Epoch 834/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0217 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.07110\n",
      "Epoch 835/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0211 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.07110\n",
      "Epoch 836/1000\n",
      "2400/2400 [==============================] - 0s 87us/step - loss: 0.0214 - val_loss: 0.0876\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.07110\n",
      "Epoch 837/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0214 - val_loss: 0.0902\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.07110\n",
      "Epoch 838/1000\n",
      "2400/2400 [==============================] - 0s 107us/step - loss: 0.0212 - val_loss: 0.0892\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.07110\n",
      "Epoch 839/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0214 - val_loss: 0.0943\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.07110\n",
      "Epoch 840/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0212 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.07110\n",
      "Epoch 841/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0214 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.07110\n",
      "Epoch 842/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0213 - val_loss: 0.0953\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.07110\n",
      "Epoch 843/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0213 - val_loss: 0.0840\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.07110\n",
      "Epoch 844/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0222 - val_loss: 0.0991\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.07110\n",
      "Epoch 845/1000\n",
      "2400/2400 [==============================] - 0s 91us/step - loss: 0.0218 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.07110\n",
      "Epoch 846/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0215 - val_loss: 0.0932\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.07110\n",
      "Epoch 847/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0216 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.07110\n",
      "Epoch 848/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0211 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.07110\n",
      "Epoch 849/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0214 - val_loss: 0.0876\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.07110\n",
      "Epoch 850/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0211 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.07110\n",
      "Epoch 851/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0213 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.07110\n",
      "Epoch 852/1000\n",
      "2400/2400 [==============================] - 0s 88us/step - loss: 0.0213 - val_loss: 0.0880\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.07110\n",
      "Epoch 853/1000\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.0212 - val_loss: 0.0844\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.07110\n",
      "Epoch 854/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0213 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.07110\n",
      "Epoch 855/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0213 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.07110\n",
      "Epoch 856/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0209 - val_loss: 0.0973\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.07110\n",
      "Epoch 857/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0217 - val_loss: 0.0964\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.07110\n",
      "Epoch 858/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0212 - val_loss: 0.0863\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.07110\n",
      "Epoch 859/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0215 - val_loss: 0.0870\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.07110\n",
      "Epoch 860/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0209 - val_loss: 0.0875\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.07110\n",
      "Epoch 861/1000\n",
      "2400/2400 [==============================] - 0s 79us/step - loss: 0.0212 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.07110\n",
      "Epoch 862/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0212 - val_loss: 0.0871\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.07110\n",
      "Epoch 863/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0208 - val_loss: 0.0867\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.07110\n",
      "Epoch 864/1000\n",
      "2400/2400 [==============================] - 0s 106us/step - loss: 0.0212 - val_loss: 0.0973\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.07110\n",
      "Epoch 865/1000\n",
      "2400/2400 [==============================] - 0s 133us/step - loss: 0.0215 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.07110\n",
      "Epoch 866/1000\n",
      "2400/2400 [==============================] - 0s 122us/step - loss: 0.0208 - val_loss: 0.0872\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.07110\n",
      "Epoch 867/1000\n",
      "2400/2400 [==============================] - 0s 110us/step - loss: 0.0212 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.07110\n",
      "Epoch 868/1000\n",
      "2400/2400 [==============================] - 0s 95us/step - loss: 0.0211 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.07110\n",
      "Epoch 869/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0214 - val_loss: 0.0932\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.07110\n",
      "Epoch 870/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0212 - val_loss: 0.0929\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.07110\n",
      "Epoch 871/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0211 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.07110\n",
      "Epoch 872/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0215 - val_loss: 0.0896\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.07110\n",
      "Epoch 873/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0208 - val_loss: 0.0891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00873: val_loss did not improve from 0.07110\n",
      "Epoch 874/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0208 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.07110\n",
      "Epoch 875/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0209 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.07110\n",
      "Epoch 876/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0211 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.07110\n",
      "Epoch 877/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0212 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.07110\n",
      "Epoch 878/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0208 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.07110\n",
      "Epoch 879/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0213 - val_loss: 0.0942\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.07110\n",
      "Epoch 880/1000\n",
      "2400/2400 [==============================] - 0s 69us/step - loss: 0.0210 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.07110\n",
      "Epoch 881/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0207 - val_loss: 0.0923\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.07110\n",
      "Epoch 882/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0211 - val_loss: 0.0962\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.07110\n",
      "Epoch 883/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0215 - val_loss: 0.0965\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.07110\n",
      "Epoch 884/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0207 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.07110\n",
      "Epoch 885/1000\n",
      "2400/2400 [==============================] - 0s 87us/step - loss: 0.0208 - val_loss: 0.0933\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.07110\n",
      "Epoch 886/1000\n",
      "2400/2400 [==============================] - 0s 81us/step - loss: 0.0211 - val_loss: 0.0818\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.07110\n",
      "Epoch 887/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0209 - val_loss: 0.0946\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.07110\n",
      "Epoch 888/1000\n",
      "2400/2400 [==============================] - 0s 104us/step - loss: 0.0209 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.07110\n",
      "Epoch 889/1000\n",
      "2400/2400 [==============================] - 0s 99us/step - loss: 0.0210 - val_loss: 0.0830\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.07110\n",
      "Epoch 890/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0212 - val_loss: 0.0987\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.07110\n",
      "Epoch 891/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0209 - val_loss: 0.1027\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.07110\n",
      "Epoch 892/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0209 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.07110\n",
      "Epoch 893/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0211 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.07110\n",
      "Epoch 894/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0214 - val_loss: 0.0910\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.07110\n",
      "Epoch 895/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0209 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.07110\n",
      "Epoch 896/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0210 - val_loss: 0.0917\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.07110\n",
      "Epoch 897/1000\n",
      "2400/2400 [==============================] - 0s 84us/step - loss: 0.0206 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.07110\n",
      "Epoch 898/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0203 - val_loss: 0.0941\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.07110\n",
      "Epoch 899/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0209 - val_loss: 0.0957\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.07110\n",
      "Epoch 900/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0209 - val_loss: 0.0793\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.07110\n",
      "Epoch 901/1000\n",
      "2400/2400 [==============================] - 0s 117us/step - loss: 0.0211 - val_loss: 0.1050\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.07110\n",
      "Epoch 902/1000\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.0209 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.07110\n",
      "Epoch 903/1000\n",
      "2400/2400 [==============================] - 0s 104us/step - loss: 0.0210 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.07110\n",
      "Epoch 904/1000\n",
      "2400/2400 [==============================] - 0s 110us/step - loss: 0.0208 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.07110\n",
      "Epoch 905/1000\n",
      "2400/2400 [==============================] - 0s 144us/step - loss: 0.0205 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.07110\n",
      "Epoch 906/1000\n",
      "2400/2400 [==============================] - 0s 111us/step - loss: 0.0204 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.07110\n",
      "Epoch 907/1000\n",
      "2400/2400 [==============================] - 0s 204us/step - loss: 0.0209 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.07110\n",
      "Epoch 908/1000\n",
      "2400/2400 [==============================] - 0s 147us/step - loss: 0.0210 - val_loss: 0.0857\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.07110\n",
      "Epoch 909/1000\n",
      "2400/2400 [==============================] - 0s 105us/step - loss: 0.0210 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.07110\n",
      "Epoch 910/1000\n",
      "2400/2400 [==============================] - 0s 84us/step - loss: 0.0205 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.07110\n",
      "Epoch 911/1000\n",
      "2400/2400 [==============================] - 0s 83us/step - loss: 0.0207 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.07110\n",
      "Epoch 912/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0208 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.07110\n",
      "Epoch 913/1000\n",
      "2400/2400 [==============================] - 0s 99us/step - loss: 0.0207 - val_loss: 0.0929\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.07110\n",
      "Epoch 914/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0204 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.07110\n",
      "Epoch 915/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0204 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.07110\n",
      "Epoch 916/1000\n",
      "2400/2400 [==============================] - 0s 88us/step - loss: 0.0207 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.07110\n",
      "Epoch 917/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0207 - val_loss: 0.0895\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.07110\n",
      "Epoch 918/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0206 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.07110\n",
      "Epoch 919/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0206 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.07110\n",
      "Epoch 920/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0203 - val_loss: 0.0858\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.07110\n",
      "Epoch 921/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0205 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.07110\n",
      "Epoch 922/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0202 - val_loss: 0.0944\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.07110\n",
      "Epoch 923/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0202 - val_loss: 0.0938\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.07110\n",
      "Epoch 924/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0207 - val_loss: 0.0898\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.07110\n",
      "Epoch 925/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0205 - val_loss: 0.0914\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.07110\n",
      "Epoch 926/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0208 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.07110\n",
      "Epoch 927/1000\n",
      "2400/2400 [==============================] - 0s 63us/step - loss: 0.0211 - val_loss: 0.0936\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.07110\n",
      "Epoch 928/1000\n",
      "2400/2400 [==============================] - 0s 64us/step - loss: 0.0209 - val_loss: 0.0861\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.07110\n",
      "Epoch 929/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0209 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.07110\n",
      "Epoch 930/1000\n",
      "2400/2400 [==============================] - 0s 89us/step - loss: 0.0209 - val_loss: 0.0929\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.07110\n",
      "Epoch 931/1000\n",
      "2400/2400 [==============================] - 0s 77us/step - loss: 0.0208 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.07110\n",
      "Epoch 932/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0207 - val_loss: 0.0932\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.07110\n",
      "Epoch 933/1000\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.0204 - val_loss: 0.0975\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.07110\n",
      "Epoch 934/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0205 - val_loss: 0.0983\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.07110\n",
      "Epoch 935/1000\n",
      "2400/2400 [==============================] - 0s 86us/step - loss: 0.0202 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.07110\n",
      "Epoch 936/1000\n",
      "2400/2400 [==============================] - 0s 94us/step - loss: 0.0208 - val_loss: 0.1044\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.07110\n",
      "Epoch 937/1000\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.0207 - val_loss: 0.0903\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.07110\n",
      "Epoch 938/1000\n",
      "2400/2400 [==============================] - ETA: 0s - loss: 0.020 - 0s 81us/step - loss: 0.0205 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.07110\n",
      "Epoch 939/1000\n",
      "2400/2400 [==============================] - 0s 112us/step - loss: 0.0201 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.07110\n",
      "Epoch 940/1000\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.0208 - val_loss: 0.0876\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.07110\n",
      "Epoch 941/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0206 - val_loss: 0.0919\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.07110\n",
      "Epoch 942/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0201 - val_loss: 0.0974\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.07110\n",
      "Epoch 943/1000\n",
      "2400/2400 [==============================] - 0s 99us/step - loss: 0.0201 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.07110\n",
      "Epoch 944/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0203 - val_loss: 0.0854\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.07110\n",
      "Epoch 945/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0203 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.07110\n",
      "Epoch 946/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0207 - val_loss: 0.0942\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.07110\n",
      "Epoch 947/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0202 - val_loss: 0.0905\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.07110\n",
      "Epoch 948/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0201 - val_loss: 0.0973\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.07110\n",
      "Epoch 949/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0200 - val_loss: 0.0911\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.07110\n",
      "Epoch 950/1000\n",
      "2400/2400 [==============================] - 0s 75us/step - loss: 0.0205 - val_loss: 0.0908\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.07110\n",
      "Epoch 951/1000\n",
      "2400/2400 [==============================] - 0s 78us/step - loss: 0.0200 - val_loss: 0.0934\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.07110\n",
      "Epoch 952/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0201 - val_loss: 0.0908\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.07110\n",
      "Epoch 953/1000\n",
      "2400/2400 [==============================] - 0s 80us/step - loss: 0.0201 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.07110\n",
      "Epoch 954/1000\n",
      "2400/2400 [==============================] - 0s 91us/step - loss: 0.0205 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.07110\n",
      "Epoch 955/1000\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.0204 - val_loss: 0.0869\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.07110\n",
      "Epoch 956/1000\n",
      "2400/2400 [==============================] - 0s 122us/step - loss: 0.0199 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.07110\n",
      "Epoch 957/1000\n",
      "2400/2400 [==============================] - 0s 145us/step - loss: 0.0204 - val_loss: 0.0893\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.07110\n",
      "Epoch 958/1000\n",
      "2400/2400 [==============================] - 0s 129us/step - loss: 0.0205 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.07110\n",
      "Epoch 959/1000\n",
      "2400/2400 [==============================] - 0s 133us/step - loss: 0.0207 - val_loss: 0.0915\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.07110\n",
      "Epoch 960/1000\n",
      "2400/2400 [==============================] - 0s 102us/step - loss: 0.0203 - val_loss: 0.0819\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.07110\n",
      "Epoch 961/1000\n",
      "2400/2400 [==============================] - 0s 88us/step - loss: 0.0204 - val_loss: 0.0946\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.07110\n",
      "Epoch 962/1000\n",
      "2400/2400 [==============================] - 0s 101us/step - loss: 0.0197 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.07110\n",
      "Epoch 963/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0200 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.07110\n",
      "Epoch 964/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0199 - val_loss: 0.0855\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.07110\n",
      "Epoch 965/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0202 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.07110\n",
      "Epoch 966/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0202 - val_loss: 0.0908\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.07110\n",
      "Epoch 967/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0203 - val_loss: 0.0901\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.07110\n",
      "Epoch 968/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0201 - val_loss: 0.0993\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.07110\n",
      "Epoch 969/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0199 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.07110\n",
      "Epoch 970/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0202 - val_loss: 0.0941\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.07110\n",
      "Epoch 971/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0201 - val_loss: 0.0970\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.07110\n",
      "Epoch 972/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0202 - val_loss: 0.0940\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.07110\n",
      "Epoch 973/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0200 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.07110\n",
      "Epoch 974/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0203 - val_loss: 0.0904\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.07110\n",
      "Epoch 975/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0198 - val_loss: 0.0879\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.07110\n",
      "Epoch 976/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0199 - val_loss: 0.0925\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.07110\n",
      "Epoch 977/1000\n",
      "2400/2400 [==============================] - 0s 91us/step - loss: 0.0200 - val_loss: 0.0954\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.07110\n",
      "Epoch 978/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0204 - val_loss: 0.0868\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.07110\n",
      "Epoch 979/1000\n",
      "2400/2400 [==============================] - 0s 90us/step - loss: 0.0198 - val_loss: 0.0941\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.07110\n",
      "Epoch 980/1000\n",
      "2400/2400 [==============================] - 0s 89us/step - loss: 0.0199 - val_loss: 0.0935\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.07110\n",
      "Epoch 981/1000\n",
      "2400/2400 [==============================] - 0s 72us/step - loss: 0.0198 - val_loss: 0.0922\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.07110\n",
      "Epoch 982/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0201 - val_loss: 0.0942\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.07110\n",
      "Epoch 983/1000\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.0200 - val_loss: 0.0890\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.07110\n",
      "Epoch 984/1000\n",
      "2400/2400 [==============================] - 0s 82us/step - loss: 0.0199 - val_loss: 0.0889\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.07110\n",
      "Epoch 985/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0197 - val_loss: 0.0926\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.07110\n",
      "Epoch 986/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0205 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.07110\n",
      "Epoch 987/1000\n",
      "2400/2400 [==============================] - 0s 71us/step - loss: 0.0205 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.07110\n",
      "Epoch 988/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0200 - val_loss: 0.0924\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.07110\n",
      "Epoch 989/1000\n",
      "2400/2400 [==============================] - 0s 68us/step - loss: 0.0197 - val_loss: 0.0888\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.07110\n",
      "Epoch 990/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0204 - val_loss: 0.0967\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.07110\n",
      "Epoch 991/1000\n",
      "2400/2400 [==============================] - 0s 65us/step - loss: 0.0202 - val_loss: 0.1002\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.07110\n",
      "Epoch 992/1000\n",
      "2400/2400 [==============================] - 0s 67us/step - loss: 0.0203 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.07110\n",
      "Epoch 993/1000\n",
      "2400/2400 [==============================] - 0s 85us/step - loss: 0.0198 - val_loss: 0.0913\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.07110\n",
      "Epoch 994/1000\n",
      "2400/2400 [==============================] - 0s 76us/step - loss: 0.0199 - val_loss: 0.0887\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.07110\n",
      "Epoch 995/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0200 - val_loss: 0.0833\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.07110\n",
      "Epoch 996/1000\n",
      "2400/2400 [==============================] - 0s 73us/step - loss: 0.0206 - val_loss: 0.0945\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.07110\n",
      "Epoch 997/1000\n",
      "2400/2400 [==============================] - 0s 66us/step - loss: 0.0200 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.07110\n",
      "Epoch 998/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0203 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.07110\n",
      "Epoch 999/1000\n",
      "2400/2400 [==============================] - 0s 70us/step - loss: 0.0199 - val_loss: 0.0928\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.07110\n",
      "Epoch 1000/1000\n",
      "2400/2400 [==============================] - 0s 74us/step - loss: 0.0198 - val_loss: 0.0941\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.07110\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "save_path = 'models/'\n",
    "filepath=save_path+\"modelQ.hdf5\"\n",
    "checkpointer = ModelCheckpoint(monitor='val_loss', filepath=filepath, verbose=1, save_best_only=True)\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=64, epochs=1000, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot metrics\n",
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='Training loss')\n",
    "pyplot.plot(history.history['val_loss'], label='Validation loss')\n",
    "\n",
    "# pyplot.plot(history.history['mean_absolute_percentage_error'])\n",
    "plt.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n",
      "11\n",
      "429/429 [==============================] - 0s 125us/step\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "print(np.argmin(np.array(history.history['val_loss']))+1)\n",
    "\n",
    "yhat = model.predict(X_test, verbose=1)\n",
    "yp = y_scaler.inverse_transform(yhat)\n",
    "ya = y_scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137.11967, 136.82777],\n",
       "       [137.04646, 136.8898 ],\n",
       "       [137.08621, 136.87053],\n",
       "       [146.834  , 147.30194],\n",
       "       [151.3965 , 154.58522],\n",
       "       [136.56416, 138.02402],\n",
       "       [136.94997, 136.87785],\n",
       "       [137.26079, 138.2727 ],\n",
       "       [145.50319, 149.33017],\n",
       "       [143.39433, 146.85472],\n",
       "       [139.80106, 145.70451],\n",
       "       [149.32594, 154.31012],\n",
       "       [151.11374, 155.38264],\n",
       "       [147.58815, 150.74994],\n",
       "       [142.19994, 142.82912],\n",
       "       [136.86713, 136.76085],\n",
       "       [138.12682, 138.54456],\n",
       "       [135.96675, 142.16504],\n",
       "       [136.06955, 140.4947 ],\n",
       "       [136.69296, 137.31842],\n",
       "       [136.93939, 136.87907],\n",
       "       [136.87059, 136.95778],\n",
       "       [136.78217, 139.94382],\n",
       "       [139.79007, 141.20644],\n",
       "       [136.77507, 137.2531 ],\n",
       "       [137.00737, 136.90605],\n",
       "       [136.62695, 139.56137],\n",
       "       [136.6208 , 138.5673 ],\n",
       "       [136.65826, 137.27592],\n",
       "       [136.70825, 137.09456],\n",
       "       [136.55214, 137.44188],\n",
       "       [137.06757, 140.06729],\n",
       "       [136.568  , 142.37117],\n",
       "       [137.09264, 138.28154],\n",
       "       [137.48763, 139.76114],\n",
       "       [138.24696, 142.11824],\n",
       "       [136.78636, 143.27109],\n",
       "       [138.49162, 141.43806],\n",
       "       [138.7159 , 141.47792],\n",
       "       [137.78838, 143.21408],\n",
       "       [137.36873, 144.4397 ],\n",
       "       [137.76648, 144.36893],\n",
       "       [136.50749, 147.62457],\n",
       "       [138.81517, 144.79515],\n",
       "       [137.62926, 145.60901],\n",
       "       [138.37831, 148.99855],\n",
       "       [137.61781, 152.7254 ],\n",
       "       [136.85782, 151.26834],\n",
       "       [136.73895, 145.15005],\n",
       "       [137.05753, 143.95467],\n",
       "       [137.3188 , 142.30885],\n",
       "       [138.03218, 143.16472],\n",
       "       [139.76923, 141.67424],\n",
       "       [138.72435, 141.80592],\n",
       "       [137.52774, 141.40935],\n",
       "       [137.47914, 144.00305],\n",
       "       [137.56642, 143.77876],\n",
       "       [137.22789, 143.72397],\n",
       "       [137.55693, 144.47069],\n",
       "       [137.31627, 142.94414],\n",
       "       [138.31668, 142.51102],\n",
       "       [135.85193, 148.59532],\n",
       "       [137.76013, 145.57411],\n",
       "       [137.07219, 143.23099],\n",
       "       [137.43896, 143.24536],\n",
       "       [136.92609, 143.52797],\n",
       "       [137.66606, 141.46321],\n",
       "       [136.93324, 139.16216],\n",
       "       [136.77278, 137.25362],\n",
       "       [137.2094 , 154.16556],\n",
       "       [140.48537, 145.18907],\n",
       "       [137.01306, 138.55385],\n",
       "       [137.30878, 138.29652],\n",
       "       [136.14192, 138.16066],\n",
       "       [138.94753, 145.69334],\n",
       "       [138.75024, 137.97679],\n",
       "       [139.141  , 138.16791],\n",
       "       [142.33861, 139.25581],\n",
       "       [140.16219, 138.95636],\n",
       "       [136.43443, 138.95656],\n",
       "       [137.82672, 137.86203],\n",
       "       [135.9041 , 138.82649],\n",
       "       [135.5959 , 137.98308],\n",
       "       [136.13112, 138.95834],\n",
       "       [136.65144, 139.29285],\n",
       "       [136.34186, 139.1397 ],\n",
       "       [138.97485, 146.45732],\n",
       "       [140.18968, 141.69441],\n",
       "       [139.56758, 143.94891],\n",
       "       [137.57289, 140.83522],\n",
       "       [138.01811, 140.18709],\n",
       "       [136.10452, 139.9093 ],\n",
       "       [135.25981, 139.01234],\n",
       "       [136.41528, 142.00911],\n",
       "       [137.32068, 139.80688],\n",
       "       [137.55675, 136.78229],\n",
       "       [140.24716, 145.64616],\n",
       "       [137.68303, 140.90549],\n",
       "       [137.24138, 147.13551],\n",
       "       [138.714  , 144.60501],\n",
       "       [139.47081, 139.2968 ],\n",
       "       [139.32254, 140.03784],\n",
       "       [140.36174, 140.58421],\n",
       "       [144.1035 , 139.77939],\n",
       "       [141.10802, 136.76434],\n",
       "       [137.40578, 137.04591],\n",
       "       [137.4308 , 136.89912],\n",
       "       [137.65984, 137.06758],\n",
       "       [139.23483, 137.16046],\n",
       "       [139.23477, 136.97687],\n",
       "       [137.40724, 136.97523],\n",
       "       [137.44504, 137.78021],\n",
       "       [139.78183, 143.31644],\n",
       "       [142.35391, 148.07259],\n",
       "       [140.02144, 147.44022],\n",
       "       [138.40105, 148.30751],\n",
       "       [140.84447, 151.53548],\n",
       "       [141.40984, 148.62448],\n",
       "       [139.86508, 146.58432],\n",
       "       [140.85435, 146.79504],\n",
       "       [141.72693, 147.05469],\n",
       "       [143.46463, 149.22372],\n",
       "       [141.71646, 148.2289 ],\n",
       "       [139.51842, 137.20377],\n",
       "       [141.05775, 137.0738 ],\n",
       "       [142.0044 , 137.18727],\n",
       "       [143.33936, 144.48822],\n",
       "       [145.2547 , 149.58289],\n",
       "       [145.7802 , 150.39566],\n",
       "       [136.59239, 142.81403],\n",
       "       [138.38431, 138.43913],\n",
       "       [138.46278, 137.97592],\n",
       "       [139.2393 , 144.38835],\n",
       "       [138.19118, 137.23785],\n",
       "       [137.6647 , 137.0639 ],\n",
       "       [137.44637, 137.04817],\n",
       "       [136.88727, 136.90785],\n",
       "       [136.85637, 136.9521 ],\n",
       "       [137.03099, 136.9416 ],\n",
       "       [136.90057, 136.94658],\n",
       "       [136.85214, 136.91512],\n",
       "       [137.04948, 136.9188 ],\n",
       "       [136.92029, 136.93144],\n",
       "       [136.84615, 136.91437],\n",
       "       [137.0424 , 136.90428],\n",
       "       [136.91954, 136.92201],\n",
       "       [136.86722, 136.89725],\n",
       "       [136.90341, 136.89487],\n",
       "       [136.91788, 136.91339],\n",
       "       [137.08147, 136.9114 ],\n",
       "       [137.01111, 136.9003 ],\n",
       "       [136.99492, 137.1433 ],\n",
       "       [136.185  , 138.06197],\n",
       "       [136.38437, 137.63615],\n",
       "       [137.00383, 136.92096],\n",
       "       [136.98215, 136.91556],\n",
       "       [136.6359 , 137.32364],\n",
       "       [136.76332, 137.48264],\n",
       "       [137.04886, 136.9232 ],\n",
       "       [136.90956, 136.98984],\n",
       "       [136.79312, 136.93216],\n",
       "       [135.95831, 138.7971 ],\n",
       "       [136.3138 , 139.06065],\n",
       "       [136.96674, 136.90074],\n",
       "       [136.8978 , 136.90395],\n",
       "       [136.88785, 136.88023],\n",
       "       [136.88647, 136.88005],\n",
       "       [136.93582, 136.8903 ],\n",
       "       [137.06189, 137.27681],\n",
       "       [136.8567 , 136.93184],\n",
       "       [136.49002, 137.47707],\n",
       "       [136.6128 , 137.44597],\n",
       "       [136.93852, 136.88905],\n",
       "       [136.2485 , 144.56337],\n",
       "       [136.80377, 143.23552],\n",
       "       [137.00716, 138.54945],\n",
       "       [135.98836, 141.60086],\n",
       "       [137.06532, 137.17978],\n",
       "       [137.11606, 136.93867],\n",
       "       [137.23813, 136.86177],\n",
       "       [137.26701, 136.84543],\n",
       "       [136.94875, 136.89877],\n",
       "       [136.8633 , 136.89027],\n",
       "       [136.93716, 136.90594],\n",
       "       [136.9493 , 136.89122],\n",
       "       [136.7342 , 137.38339],\n",
       "       [136.88345, 136.88452],\n",
       "       [136.87207, 136.89655],\n",
       "       [136.8409 , 136.8861 ],\n",
       "       [136.84523, 136.9082 ],\n",
       "       [136.29094, 138.15732],\n",
       "       [137.02539, 136.87846],\n",
       "       [136.87744, 136.86565],\n",
       "       [136.4504 , 137.66924],\n",
       "       [137.87819, 149.29126],\n",
       "       [155.79349, 151.23524],\n",
       "       [155.01   , 149.97653],\n",
       "       [152.31763, 148.39783],\n",
       "       [158.67029, 152.19058],\n",
       "       [159.27998, 152.25674],\n",
       "       [155.78131, 151.8686 ],\n",
       "       [160.53358, 155.71971],\n",
       "       [160.59523, 154.56421],\n",
       "       [147.26959, 141.53644],\n",
       "       [137.01678, 136.82611],\n",
       "       [136.9695 , 136.86732],\n",
       "       [137.10963, 136.89322],\n",
       "       [136.7209 , 137.69148],\n",
       "       [137.158  , 136.86482],\n",
       "       [137.17795, 136.8478 ],\n",
       "       [136.97714, 136.86504],\n",
       "       [136.77034, 136.88414],\n",
       "       [136.61687, 137.1042 ],\n",
       "       [136.8774 , 136.94968],\n",
       "       [137.12068, 136.88231],\n",
       "       [137.22163, 136.85231],\n",
       "       [137.39922, 138.15755],\n",
       "       [137.18524, 137.5186 ],\n",
       "       [137.26413, 136.85605],\n",
       "       [137.34502, 137.66365],\n",
       "       [137.54521, 137.01836],\n",
       "       [137.08873, 136.84303],\n",
       "       [137.21799, 136.8628 ],\n",
       "       [137.1843 , 136.87721],\n",
       "       [136.89938, 136.88745],\n",
       "       [136.7915 , 136.85382],\n",
       "       [136.83545, 136.90564],\n",
       "       [136.8068 , 136.91318],\n",
       "       [136.93562, 136.90688],\n",
       "       [136.88419, 136.84937],\n",
       "       [136.82306, 136.88994],\n",
       "       [136.84764, 137.02873],\n",
       "       [136.81157, 137.2139 ],\n",
       "       [137.7488 , 139.43811],\n",
       "       [137.22473, 136.89592],\n",
       "       [136.81439, 136.82536],\n",
       "       [136.98613, 136.8915 ],\n",
       "       [137.16187, 137.12506],\n",
       "       [137.24121, 136.83914],\n",
       "       [137.24406, 137.08488],\n",
       "       [137.16173, 137.5446 ],\n",
       "       [137.19553, 137.82011],\n",
       "       [137.02036, 136.8979 ],\n",
       "       [136.90816, 136.89612],\n",
       "       [137.12602, 136.87936],\n",
       "       [136.90419, 136.90076],\n",
       "       [136.96133, 136.873  ],\n",
       "       [137.35884, 138.06624],\n",
       "       [137.03526, 137.46516],\n",
       "       [137.11508, 136.8575 ],\n",
       "       [137.31097, 137.66371],\n",
       "       [136.97948, 136.86174],\n",
       "       [136.98999, 136.87466],\n",
       "       [136.85385, 136.86014],\n",
       "       [137.02522, 136.9879 ],\n",
       "       [137.12529, 136.88824],\n",
       "       [137.04666, 137.25151],\n",
       "       [136.86069, 136.8785 ],\n",
       "       [136.98665, 136.84576],\n",
       "       [137.1952 , 136.86504],\n",
       "       [137.01897, 136.89268],\n",
       "       [137.05586, 136.8915 ],\n",
       "       [136.88411, 136.91449],\n",
       "       [136.84169, 136.91487],\n",
       "       [136.95305, 136.90157],\n",
       "       [137.04192, 136.91176],\n",
       "       [137.0495 , 136.91321],\n",
       "       [136.91289, 136.88756],\n",
       "       [136.89398, 136.8922 ],\n",
       "       [136.99185, 136.87538],\n",
       "       [136.9251 , 136.8852 ],\n",
       "       [136.85649, 136.9106 ],\n",
       "       [136.96594, 136.8967 ],\n",
       "       [136.96321, 136.86546],\n",
       "       [136.7823 , 136.90633],\n",
       "       [136.83133, 136.92189],\n",
       "       [137.0381 , 136.87328],\n",
       "       [136.8761 , 136.90399],\n",
       "       [136.90514, 136.91737],\n",
       "       [136.95844, 136.8981 ],\n",
       "       [136.91016, 136.91425],\n",
       "       [136.8439 , 136.91618],\n",
       "       [137.0314 , 136.8634 ],\n",
       "       [137.32431, 137.33691],\n",
       "       [137.09438, 136.88197],\n",
       "       [136.58348, 137.48991],\n",
       "       [136.9718 , 136.88683],\n",
       "       [137.01828, 136.89116],\n",
       "       [137.0505 , 136.89537],\n",
       "       [137.04135, 136.89316],\n",
       "       [137.00314, 136.8862 ],\n",
       "       [137.01207, 136.89076],\n",
       "       [137.03665, 136.88138],\n",
       "       [136.95961, 136.91054],\n",
       "       [137.09016, 136.92868],\n",
       "       [137.03279, 136.8882 ],\n",
       "       [136.92119, 136.90157],\n",
       "       [137.04153, 136.87048],\n",
       "       [137.03204, 136.88362],\n",
       "       [136.96478, 136.89343],\n",
       "       [136.97533, 136.8916 ],\n",
       "       [137.00441, 136.88103],\n",
       "       [136.98871, 136.88075],\n",
       "       [136.9613 , 136.88591],\n",
       "       [136.90329, 136.88391],\n",
       "       [136.92697, 136.87929],\n",
       "       [136.90465, 136.8811 ],\n",
       "       [136.91856, 136.88387],\n",
       "       [136.9271 , 136.88982],\n",
       "       [136.92393, 136.89238],\n",
       "       [136.93369, 136.88852],\n",
       "       [137.01088, 136.88504],\n",
       "       [137.07928, 136.87794],\n",
       "       [137.0942 , 136.8863 ],\n",
       "       [137.0607 , 136.88599],\n",
       "       [137.03102, 136.87845],\n",
       "       [136.97162, 136.88945],\n",
       "       [137.00438, 136.8689 ],\n",
       "       [137.05576, 136.84448],\n",
       "       [137.03673, 136.88387],\n",
       "       [137.05193, 136.87813],\n",
       "       [136.94402, 136.88708],\n",
       "       [136.9435 , 136.88655],\n",
       "       [136.89618, 136.88358],\n",
       "       [136.90895, 136.88493],\n",
       "       [137.00447, 136.86974],\n",
       "       [136.87575, 136.89407],\n",
       "       [136.89351, 136.88486],\n",
       "       [137.00081, 136.87883],\n",
       "       [136.97612, 136.87978],\n",
       "       [136.97435, 136.87993],\n",
       "       [136.96426, 136.88194],\n",
       "       [136.93513, 136.88358],\n",
       "       [136.9619 , 136.88535],\n",
       "       [136.96085, 136.88498],\n",
       "       [136.9463 , 136.88702],\n",
       "       [136.92197, 136.89003],\n",
       "       [136.90993, 136.88824],\n",
       "       [136.99702, 136.8901 ],\n",
       "       [137.00197, 136.88715],\n",
       "       [136.96237, 136.88689],\n",
       "       [136.96162, 136.88565],\n",
       "       [136.88272, 136.8887 ],\n",
       "       [136.88295, 136.88689],\n",
       "       [136.93675, 136.88364],\n",
       "       [136.96567, 136.88817],\n",
       "       [136.95537, 136.89584],\n",
       "       [137.01057, 136.89005],\n",
       "       [137.00882, 136.8903 ],\n",
       "       [137.02206, 136.88812],\n",
       "       [137.0078 , 136.87358],\n",
       "       [136.9516 , 136.88446],\n",
       "       [136.99507, 136.8955 ],\n",
       "       [137.00774, 136.89859],\n",
       "       [137.00774, 136.8986 ],\n",
       "       [136.96652, 136.89299],\n",
       "       [136.96657, 136.89346],\n",
       "       [136.81091, 137.06126],\n",
       "       [136.88167, 136.98259],\n",
       "       [136.95262, 136.89421],\n",
       "       [136.196  , 137.73093],\n",
       "       [136.32486, 137.58798],\n",
       "       [136.95427, 136.89378],\n",
       "       [136.96109, 136.89313],\n",
       "       [136.75804, 137.11975],\n",
       "       [136.97687, 136.89986],\n",
       "       [136.98296, 136.89214],\n",
       "       [137.00955, 136.90532],\n",
       "       [136.95601, 136.90948],\n",
       "       [136.07112, 140.88547],\n",
       "       [136.0723 , 140.62074],\n",
       "       [136.91058, 136.98323],\n",
       "       [136.83302, 137.07948],\n",
       "       [136.45728, 137.5066 ],\n",
       "       [136.47325, 137.65509],\n",
       "       [136.6259 , 137.29823],\n",
       "       [136.52533, 141.12231],\n",
       "       [136.40263, 141.5585 ],\n",
       "       [136.18007, 145.06897],\n",
       "       [136.12389, 145.17809],\n",
       "       [135.98859, 142.66975],\n",
       "       [136.3125 , 138.68637],\n",
       "       [136.63385, 137.5117 ],\n",
       "       [136.87155, 137.01614],\n",
       "       [136.75815, 137.16327],\n",
       "       [136.96718, 136.89668],\n",
       "       [136.96523, 136.88156],\n",
       "       [136.77728, 139.82986],\n",
       "       [136.6903 , 139.40459],\n",
       "       [136.93298, 136.94215],\n",
       "       [136.96677, 136.90218],\n",
       "       [136.97993, 136.91376],\n",
       "       [136.64355, 139.7753 ],\n",
       "       [136.79927, 140.80753],\n",
       "       [136.91078, 139.80338],\n",
       "       [136.99   , 138.00523],\n",
       "       [136.98376, 139.3841 ],\n",
       "       [136.77234, 145.25056],\n",
       "       [136.96834, 145.32173],\n",
       "       [136.84402, 141.09444],\n",
       "       [136.57169, 140.53148],\n",
       "       [137.50296, 141.74716],\n",
       "       [137.34645, 141.77827],\n",
       "       [137.93755, 145.95534],\n",
       "       [138.74297, 145.2444 ],\n",
       "       [138.44269, 142.62462],\n",
       "       [136.90685, 146.3076 ],\n",
       "       [137.09897, 145.11327],\n",
       "       [137.30028, 141.0732 ],\n",
       "       [135.78624, 138.27863],\n",
       "       [135.69864, 138.20164],\n",
       "       [135.63603, 137.24576],\n",
       "       [135.37845, 137.59827],\n",
       "       [135.60341, 137.3756 ],\n",
       "       [135.74178, 140.64493],\n",
       "       [135.7464 , 137.46779],\n",
       "       [135.59602, 136.29655],\n",
       "       [135.74556, 135.49574],\n",
       "       [135.74956, 136.2025 ],\n",
       "       [135.89543, 137.4716 ],\n",
       "       [135.57878, 137.1354 ],\n",
       "       [135.68283, 136.70886],\n",
       "       [136.30527, 132.92923],\n",
       "       [136.88145, 141.72472],\n",
       "       [138.01294, 144.30746],\n",
       "       [136.99171, 136.92944],\n",
       "       [136.75053, 136.94524],\n",
       "       [138.19882, 143.6313 ],\n",
       "       [137.90382, 145.8219 ]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(429, 2)\n",
      "(429, 2)\n"
     ]
    }
   ],
   "source": [
    "print(yp.shape)\n",
    "print(ya.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f4082f7fc8>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# T1\n",
    "plt.plot(ya[:,-1:])\n",
    "plt.plot(yp[:,-1:],marker='o')\n",
    "\n",
    "# T2\n",
    "# plt.plot(ya[:,-1:])\n",
    "# plt.plot(yp[:,-1:],marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.427950807887072"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(yp, ya))\n",
    "rms_total = (rms*100)/155\n",
    "rms_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405, 1) (405, 1)\n",
      "(405, 2)\n"
     ]
    }
   ],
   "source": [
    "actual = pd.DataFrame(ya[:,-1:])\n",
    "pred = pd.DataFrame(yp[:,-1:])\n",
    "print(actual.shape, pred.shape)\n",
    "com = pd.concat([actual, pred], axis=1, join='inner')\n",
    "com.columns = ['Act_T2','Pred_T2']\n",
    "print(com.shape) \n",
    "com.to_csv('model1_T2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405, 1) (405, 1)\n",
      "(405, 2)\n"
     ]
    }
   ],
   "source": [
    "actual = pd.DataFrame(ya[:,:1])\n",
    "pred = pd.DataFrame(yp[:,:1])\n",
    "print(actual.shape, pred.shape)\n",
    "com = pd.concat([actual, pred], axis=1, join='inner')\n",
    "com.columns = ['Act_T1','Pred_T1']\n",
    "print(com.shape)\n",
    "com.to_csv('model1_T1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
